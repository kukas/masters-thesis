\chapter{Method}
\label{chap:method}

% In this chapter, we discuss the methodology of the thesis. We motivate and specify the research question, state the hypothesis and the specify how we test it.

% Methodology

% what do we want to measure? The effects of tokenization over the languages, focus on the differences the methods make on low resource vs high resource and how this translates to downstream tasks.b

% - data sampling
%     - !! do data hodně vysvětlit, že u toknizerů používáme různá alpha ale u pretrainingu ne.

% - introduce the metrics
%     - obecně naše metriky předpokládají nějaký předem daný vocabulary budget, který chceme spravedlivě rozdělit
%     - CPT, AR, JSD
%         - k AR metrice: je to samozřejmě závislý na vocab size, ale měl bych to poznamenat
%         - výhoda AR oproti CPT - u čínštiny započítává jednotlivé znaky, což by u CPT nebylo vidět

%         - k JSD metrice: není zřejmé jestli chceme vyšší overlap nebo nižší

%     - interpretation of the metrics - CPT, AR high good, JSD not sure
%     - also mention UNK and alphabet size
% - kinds of experiments
%     - tokenizers only
%         - eg: training data size, alphabet size, data imbalance
%     - tokenizers + MLM training
%         - eg: Huggingface tokenizers, replications
    
% - introduce the evaluation procedure
%     - intrinsic evaluation
%         - overall metrics - why macro average over languages
%             - how to average JSD - all pairs
%         - per language metrics - why delta from some baseline, metrics are different for different languages so we need to normalize
%     - extrinsic evaluation
%         - in-language / cross-language
%             - overall - why macroaverage, per-language, how to do per-language in cross-lingual where there are pairs (we focus on target results)
%             - seeds, averaging, bootstrapping
            %     - on how to compute significance for the cross-lingual tasks
            %         - we want to compare the average over languages
            %         - we can use bootstrapping to compute the confidence intervals
            %             - for each language select a random seed and compute the average over these seeds
            %             - sample many times and compute the confidence intervals
%         - probing vs finetuningn
%             - vysvětlit proč jsou všechny experimenty probing a jak se to liší a proč jsme to vybrali
%         - tasks NLI, NER, POS tagging, ...
%     - correlation between intrinsic and extrinsic
%         - how to compute the correlation
%         - míra korelace závisí na tom, jak moc se liší experimenty. Když porovnáváme jen stejné experimenty, pak nám vyjde nízká


In this chapter, we introduce the methodology for experiments conducted in this thesis. We introduce the important data sampling method utilized in \citet{devlin_bert_2019,conneau_unsupervised_2020} we use frequently for our experiments. Then we introduce our proposed metrics for measuring the \textit{vocabulary allocation} and \textit{vocabulary overlap} of a tokenizer. We introduce the types of experiments we will conduct --- namely we will train different tokenizers and evaluate them against our metrics. Moreover, we will also use the tokenizers to train masked language models to verify that our metrics are useful for assessing the tokenizer quality for use in language models. We will also describe in detail the evaluation procedures and all evaluation settings and tasks. 


% For all experiments we use the CC100 dataset used in \citet{conneau_unsupervised_2020}. Using the data we create multilingual vocabularies with the same size of 120K unique tokens using different methods. We use intrinsic evaluation framework from \citet{limisiewicz_tokenization_2023} to evaluate the tokenizers. Then we use the tokenizers to train multilingual masked language models on the same data and evaluate them on the same set of downstream tasks. We compare the results of the intrinsic and extrinsic evaluation to see if the improvements in the intrinsic evaluation translate to improvements in the downstream tasks.

% In this thesis, we investigate the effect of tokenizer properties on multilingual language models. We define metrics that measure the properties of the tokenizers and then we define the method by which we assess how these properties affect the performance of the language models. At the same time we want to improve the performance of the multilingual language model for the low-resource languages as this was shown to be a problem in previous work \cite{rust_how_2021}. Therefore we use the metrics we define to assess the methods proposed to solve this problem. Furthermore we propose methods to improve the performance of the language models and evaluate them using the same metrics.


\section{Data sampling}

% This subset of CC100 is then used for further experiments with vocabulary creation that will be described in the following sections.
% \xxx{TODO: describe the resampling method, compare different alphas used in the literature}
% For pretraining the models and training the tokenizers we do not use the full 10\% of the data. Because the data is heavily skewed towards the high-resource languages. Instead, we further subsample the data, following \citet{conneau_unsupervised_2020-1} to balance the number of lines per language. This is a standard practice followed by multiple independent authors \xxx{cite}. The empirical probability of sampling a line from language $l$ is given by:

% In the following chapters, we will reference often the balancing factor $\alpha$, here we defin

As explained in the previous chapters, the training data available for each language differs significantly in the total size (counted as the number of lines). For training the multilingual language model and associated tokenizer, it is generally advised to address this data imbalance by oversampling the languages with a low amount of data available (low-resource languages) and undersample the languages with high amounts of data (high-resource languages). One possible balancing procedure proposed by \citet{devlin_bert_2019,conneau_unsupervised_2020} is parametrized by the exponent $\alpha$ which we will now describe. In the following chapters, we will reference often the balancing factor $\alpha$. If not mentioned otherwise, what we mean is the data balance in the \textbf{tokenizer} training data (not pretraining data for the model) implied by sampling with the $\alpha$ parameter.

We assume we have $N$ monolingual corpora $C_l$ with languages $l \in L$. Each corpus with the language $l \in L$ has a different number of lines $N_l = |C_l|$. Then, the probability of sampling a line from the concatenation of all corpora $\cup_{l \in L} C_l$ is:

\begin{equation}
    p(l) = \frac{N_l}{\sum_{l' \in L} N_{l'}}
\end{equation}

% Where $N_l$ is the number of lines in the dataset for language $l$.

To ensure that the low-resource languages are not underrepresented in the training data, we modify this probability distribution using an exponential smoothing parameter $\alpha$:

\begin{equation}
    p'(l) = \frac{p(l)^\alpha}{\sum_{l' \in L} p(l')^\alpha}
\end{equation}

For $\alpha = 0.0$ we get a uniform distribution over the languages, for $\alpha = 1.0$ we get the original distribution. 

For pretraining the language models, we use $\alpha = 0.3$ as suggested by \citet{conneau_unsupervised_2020-1}. For training the tokenizers, we always specify the alpha as a parameter of the tokenizer training procedure. 

\section{Tokenizer metrics}

% \xxx{lets write the motivation here and maybe move it to the background section later}  

% - motivation: we want to evaluate the tokenizers before costly pretraining
% - by comparing the tokenizers we can select the best one for pretraining
% - we can also use the metrics to study the effect of the hyperparameters and other factors on the tokenizer
% - We use the metrics throughout the thesis to measure the tokenizers
% - We can measure how the tokenizer output differs between languages
%     - Explain how to measure individual languages with the same tokenizer. Metric may be a function of the tokenizer and language coded hold out data

% - Why do we want to measure the tokenizers?
%     - we want to select the best tokenizer for pretraining
%     - how do the tokenizers differ?
%         - overlap between languages - can be beneficial for some tasks (ner) but detrimental for others (pos)
%         - how much do they split words? (Rust)
%         - how much do they split sentences? (Limisiewicz)
%             - too much splitting - the model needs to learn to reconstruct words
%             - too little splitting - there is not enough examples for the model to learn from (but maybe this is not a problem for masked LM, only for machine translation)
%         - we can also measure the compatiblity of the segmentations (Maronikolakis)
%         - vocabulary capacity per language
%             - are the languages represented equally in the vocabulary?
%             - this is hard because some languages need more tokens than others (Chinese vs English)

% - how to measure tokenizers
%     - we want to measure the per-language and cross-lingual phenomena
%     - we therefore need to measure the tokenizers using language-coded data
%     - the basis for measurement will be the tokenization of the language-coded data and the empirical distribution of the tokens

%     - Average Rank - we want to measure how many tokens are used in the vocabulary - high frequency tokens are penalized because they move the average down
%         - another motivation - AR is similar to "the number of tokens needed to cover x% of the data" but it is non-parametric and includes the information about the distribution of the tokens
%     - Characters per Token - more characters = less ambiguity = better
%         - similar to word fertility, equivalent to tokens per sentence
%             - tokens per sentence used in Chung (they call it description length), Liang
%         - usable also in the langugaes without spaces
%     - of course both measures depend on the langugage

%     - interesting: ALP === entropy * avg_sequence_length

%     - overlap - what is the overlap between two languages
%         - we can look at the number of tokens that are shared between the languages but this is not that informative because these tokens might be very rare
%             - Wu and Dredze did this
%         - better is to look at the distributions of the tokens in each language and measure the similarity
%         - we can use the Jensen-Shannon divergence to measure the similarity of the distributions
%         - Chung used the Wasserstein distance but this is a distance function defined between probability distributions on a given metric space. (probability measures). We do not see how to interpret the token distribution as a probability measure and Chung does not discuss this

In this section we introduce the metrics that we use to evaluate the tokenizers. By measuring the tokenizers we would like to explore two questions. First, we would like to analyse how the tokenizers differ between each other. How granular is the segmentation given an example text? And how much are the tokens shared between the languages? Second, using the observed differences between tokenizers, we would like to analyse how they influence the multilingual language models, that are trained using the tokenizers. We will describe the methodology for measuring the influences in the following sections \xxx{ref}.

When assessing the multilingual tokenizers, we also want to focus not only on the overall properties but also investigate the quality of tokenization for the individual languages. This gives us a better understanding of the tokenizers and allows us to compare the tokenizers with each other given a target language. For this purpose, we will use monolingual evaluation corpora for each language. The metrics we define will be therefore functions of the tokenizer $\tau$ and the corpus $C_l$ with the selected language $l$. 

We introduce three metrics - average rank, characters per token and Jensen-Shannon divergence. The first two metrics aim to measure the "vocabulary allocation" of the tokenizer --- the degree to which is the given language represented in the vocabulary. The third metric measures the "vocabulary overlap" between a given pair of languages --- the degree of token sharing between two languages.

Later, in \autoref{sec:metric_comparison}, we compare our metrics with the metrics used in the literature.

To define the metrics formally, we use the following notation \cite{zouhar_tokenization_2023}. Let $\Sigma$ be a set of characters we call the alphabet. In our context, the alphabet is the set of all valid Unicode characters. We call a string $\sigma \in \Sigma^*$ a line. Finally, we call a multiset of lines $C_l = \{ \sigma_1, \ldots, \sigma_{N_l} \} \subset \Sigma^*$ a corpus of size $N_l$. The $l \in L$ denotes a language of the corpus from a set of languages $L$. Next, we denote the set $V_\tau \subset \Sigma^*$ as the vocabulary of a tokenizer $\tau$. The tokenizer $\tau: \Sigma^* \rightarrow V_\tau^*$ is a mapping from a line $\sigma \in \Sigma^*$ to a sequence of tokens $\tau(\sigma) \in V_\tau^*$. We denote the number of tokens in a sequence $s$ as $|s|$. Finally, we denote the number of occurrences of a token $v \in V_\tau$ in a corpus $C_l$ as $\textrm{cnt}(v, C_l)$.
\xxx{TODO: Check the notation}
% For each metric we will provide motivation and comparison with similar metrics used in the literature. 

% \xxx{mention the vocabulary allocation and vocabulary overlap}
\xxx{TODO: add UNK rate?!}

\subsection{Characters per token}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{img/temp/cpt_example.png}
    \caption{Example of CPT metric.}
    \label{fig:cpt_example}
\end{figure}

The first metric we propose is the average number of characters per token (CPT). The motivation for this metric is that we want to measure how granular the tokenization for a given language is. If the tokenizer splits the words into many tokens, the average number of characters per token will be low. On the other hand, if the tokenizer does not split the words, the average number of characters per token will be high. We hypothesize, that longer tokens are better for the language models, because they potentially carry more meaning. The extreme case of this metric is the character-level tokenization, where the average number of characters per token is 1. In this case the model would need to learn to reconstruct the words from the characters.

The metric is defined as follows. Given a tokenizer $\tau$ and a language corpus $C_l$, we first tokenize the corpus using the tokenizer $\tau$. Then we compute the average number of characters per token in the tokenized corpus:

\begin{equation}
    CPT(\tau, C_l) = \frac{\sum_{s \in C_l}|s|}{\sum_{s \in C_l}|\tau(s)|}
\end{equation}

where $|s|$ is the number of characters in the sentence $s \in C_l$ and $|\tau(s)|$ is the number of tokens in the tokenized sentence. The metric is illustrated in Figure \ref{fig:cpt_example}.

\subsection{Average Rank}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.6\textwidth]{img/temp/ar_example.png}
    \caption{Example of AR metric.}
    \label{fig:ar_example}
\end{figure}

Another metric we use for comparing the tokenizers is Average Rank (AR). The motivation for this metric is that we want to measure how many tokens are effectively used in the vocabulary for representing the corpus. Each language will have some amount of tokens dedicated to it in the vocabulary and our goal is to measure this allocation. We also want to take into account how frequently are these tokens used. We hypothesize that very frequent and very rare tokens are not as useful for the language models as the high-frequency tokens might be too ambiguous and low-frequency tokens might not have enough training examples to learn from \cite{gowda_finding_2020}. We therefore propose to measure the average rank (the position of the token sorted by frequency) of tokens needed to cover a monolingual corpus, weighted by their probability.
% Here one metric suggestion could be something along the lines of "number of tokens from vocabulary of $\tau$ needed to cover the 95/98/100\% of the corpus". That metric would work but the downside is that we do not know how to set the threshold. Moreover we would like the metric to reflect the shape of the distribution of the token coverage. We prefer a distribution that is more balanced. High-frequency tokens might be too ambiguous and low-frequency tokens might not have enough training examples to learn from \cite{gowda_finding_2020}. Therefore we propose a different metric that reflects both the vocabulary allocation and the uniformity of the token distribution. On top of that it is parameter-free.

Given a tokenizer $\tau$ and a language corpus $C_l$, we first tokenize the corpus using the tokenizer $\tau$. Then we compute the empirical probability of the tokens in the tokenized corpus.

\begin{equation}
    \hat{p}_{\tau(C_l)}(t) = \frac{count(t, \tau(C_l))}{\sum_{t' \in \tau(C_l)} count(t', \tau(C_l))}
\end{equation}

We sort the tokens by their probability and assign them ranks from 1 to $|V_\tau|$. The average rank is then the weighted average of the ranks of the tokens, where the weights are the probabilites of the tokens:

\begin{equation}
    AR(\tau, C_l) = \sum_{t \in V_\tau} rank(t, \tau(C_l)) \cdot \hat{p}_{\tau(C_l)}(t)
\end{equation}

The metric is illustrated in Figure \ref{fig:ar_example}. Higher AR signals that the vocabulary contains higher number of tokens used for tokenizing given language. Moreover with high AR we can expect that the tokens are distributed more uniformly.

\subsection{Jensen-Shannon Divergence}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.5\textwidth]{img/temp/jsd_example.png}
    \caption{Jensen-Shannon Divergence}
    \label{fig:jsd_example}
\end{figure}

The Jensen-Shannon Divergence (JSD) is a metric that measures the similarity between two probability distributions. It is defined as follows:

\begin{equation}
    JSD(p, q) = \frac{1}{2} \cdot (KL(p||m) + KL(q||m))
\end{equation}

Where $m = \frac{1}{2} \cdot (p + q)$ is the midpoint distribution and $KL(p||q)$ is the Kullback-Leibler divergence. 

\begin{equation}
    KL(p||q) = \sum_{t \in V_\tau} p(t) \log \frac{p(t)}{q(t)}
\end{equation}

We will use JSD for the analysis of an overlap between two languages given a tokenizer. Tokenization of two monolingual corpora $C_{l_1}$ and $C_{l_2}$ with the same tokenizer $\tau$ will result in two probability distributions over the vocabulary $V_\tau$. We will denote these distributions as $\hat{p}_{\tau(C_{l_1})}$ and $\hat{p}_{\tau(C_{l_2})}$. 

Computing the JSD between the two distributions will result in a metric that measures how much the two distributions overlap. The JSD is a symmetric metric that is bounded between 0 and 1. Low JSD means that the two distributions are similar, high JSD means that the two distributions are different. 

\subsection{Alphabet size and out-of-vocabulary tokens}

Among the metrics we propose, we also measure the alphabet size of the tokenizers defined as the number of tokens of length 1 in the vocabulary:

\begin{equation}
    \mathrm{Alphabet} = |\{t \in V_\tau, |t| = 1\}|
\end{equation}

We also measure the number of out-of-vocabulary (OOV) tokens in the corpus. The UNK token is a special token that used to represent all the tokens that are not present in the vocabulary $V_\tau$. We measure the number of UNK tokens in the corpus as follows:

\begin{equation}
    \mathrm{OOV} = \sum_{t \in \tau(C_l)} \mathbbm{1}_{t = \mathrm{<UNK>}}
\end{equation}

Where $\mathbbm{1}_{t = \mathrm{<UNK>}}$ is an indicator function that is 1 if the token is UNK and 0 otherwise. Because we use the same validation set for all the tokenizers, we can compare the number of OOV tokens directly between the tokenizers. Note that in the literature, the out-of-vocabulary tokens are measured as a OOV rate (number of OOV tokens divided by the total number of tokens in the corpus). We do not use this metric because the values of the OOV rate are small for the subword tokenizers and therefore slightly harder to compare.

% Further, we will be mainly concerned with the tokenization of the training corpora $\tau(C_l)$ and the empirical distribution over the tokenizer vocabulary:

% \xxx{fix the equation once I settle on the notation. The denominator is wrong here}

% - test the tokenizer metrics, show correlation between CPT, AR and JSD -> downstream
%     - define the metricshe implementation}

% In \cite{limisiewicz_tokenization_2023} we have compared the Unigram LM and BPE tokenizers. We have found that generally, the BPE tokenizer performs better on the tokenizer intrinsic metrics and the downstream tasks. During the work on the thesis we have found that this finding might have been heavily influenced by the choice of the implementation of the Unigram LM training algorithm. In \cite{limisiewicz_tokenization_2023} we have used the Huggingface implementation of the tokenizers. In the later experiments we have used the Sentencepiece implementation. We have found that the Sentencepiece implementation produces tokenizers that close the gap in the intrinsic evaluation. We have therefore decided to use the Sentencepiece implementation for all experiments in this thesis.

% \xxx{add image}

% \xxx{TODO: add the other factors - co
%         - CPT
%         - AR
%         - JSD
%     - compare 4 tokenizers
%         - vanilla Unigram, vanilla BPE
%         - NoOverlap - to study the effect of the overlap
%         - TokMix - to ensure a uniform 
%     - experiments for 6 languages
%     - experiments for 20 languages

% \section{Investigating the influence of the tokenizer on the language model}

\section{Evaluation procedures}

In this section, we introduce the evaluation procedures that we use to evaluate the tokenizers. We first describe the types of experiments we do with the tokenizers. Then we explain the intrinsic evaluation procedure. Then we describe the extrinsic evaluation procedure.

\subsection{Types of experiments}

Generally, we can distinguish two types of experiments we do with the tokenizers we compare in this thesis. The first type of experiment is comparing different tokenizers to each other using the evaluation metrics we introduced in the previous section. For example, we can compare the Unigram LM tokenizer to the BPE tokenizer. To do this, we will train the tokenizers on the same training corpus and then evaluate them using the intrinsic evaluation metrics on validation sets from all the languages $L$. 

The second type of experiment is comparing different tokenizers on downstream tasks. For example, we can compare the Unigram LM tokenizer to the BPE tokenizer on the task of natural language understanding. To do this, we will train the tokenizers on the same training corpus and then use the tokenizers to train otherwise identical language models. Then we will evaluate the language models on the same validation sets from all the languages $L$. Because the language models are identical, the differences in the performance will be caused only by the differences in the tokenizers and the random initialization of the language models.

\subsection{Intrinsic evaluation}

For the intrinsic evaluation of a set of tokenizers, we compute the metrics we introduced in the previous section on the validation sets from all the languages $L$. We compute the metrics for each tokenizer and validation language separately. Then, we compare the per language metrics between the tokenizers. Because the metrics are generally different for different languages (for example the characters per token for Chinese will be always smaller than the characters per token for English), we will compare the differences between the tokenizers rather than the absolute values.

We will also compute the metrics for the whole set of languages $L$. We do this by averaging the metrics over all the languages. We use the macro average, which means that we average the metrics for each language with the same weight. We use the macro average because we want to assess equally the improvements over the high-resource and low-resource languages. This is of course an arbitrary choice we make to assess each language equally. Different weighting schemes might be more appropriate for different applications.

The JSD metric which measures the \textit{vocabulary overlap} is defined between a pair of languages, measuring the overlap between the two. We compute the overall overlap by considering the JSD values for all pairs $l_1, l_2 \in L, l_1 \neq l_2$.

\subsection{Extrinsic evaluation}
%     - extrinsic evaluation
%         - in-language / cross-language
%             - overall - why macroaverage, per-language, how to do per-language in cross-lingual where there are pairs (we focus on target results)
%             - seeds, averaging, bootstrapping
            %     - on how to compute significance for the cross-lingual tasks
            %         - we want to compare the average over languages
            %         - we can use bootstrapping to compute the confidence intervals
            %             - for each language select a random seed and compute the average over these seeds
            %             - sample many times and compute the confidence intervals
%         - probing vs finetuningn
%             - vysvětlit proč jsou všechny experimenty probing a jak se to liší a proč jsme to vybrali
%         - tasks NLI, NER, POS tagging, ...
%     - correlation between intrinsic and extrinsic
%         - how to compute the correlation
%         - míra korelace závisí na tom, jak moc se liší experimenty. Když porovnáváme jen stejné experimenty, pak nám vyjde nízká




% \tomasz{Here, I would write section Extrinsic Evaluation. 
% With general description assesing tokenizer influnce on downstream tasks.
% Details should be provided in the next chapter: Experiments...}

