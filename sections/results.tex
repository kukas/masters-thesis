
\chapter{Results}
\section{Preliminary experiments}
\subsection{The importance of training data size}
\subsection{Differences in tokenizer implementations}

\section{Reproduction of baselines}
\section{Document-level clustering method}
\section{Extrinsic evaluation}


- replication of the previous work
    - Chung
        - reproductions in Overlap-based Vocabulary Generation Improves Cross-lingual Transfer Among Related Languages

    - Liang
        - they use 900k vocab, they compare their model to XLM-R which is not fair!
            - they discuss it in section 6.4
        - reproductions: https://github.com/stefan-it/xlm-v-experiments
            - For XQuAD they did not reproduce the improvements
            - For MasakhaNER they reproduced the improvements
            - TODO: could use bootstrapping to show whether the improvements are significant

replications of Chung, Liang
https://github.com/stefan-it/xlm-v-experiments

- our beta experiments point to the randomness of the output - the smooth sweep across the beta values seems to produce quite noisy outpus