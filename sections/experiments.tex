
\chapter{Experimental setup}

% \textbf{Q1:} How do subword tokenizers differ in overlap and allocation of learned vocabularies?
% \textbf{Q2:} Which properties of multilingual tokenizers affect the language model representation quality?
% \textbf{Q3:} What is the reason that the standard tokenizer training method does not work well in the multilingual setting? 
% \textbf{Q4:} What is the effect of using the reproduced methods on the representation of low-resource languages? And
% \textbf{Q5:} How do the reproduced methods compare to the standard method of training the tokenizer on balanced and unbalanced joint corpus?

In this chapter, we introduce a common experimental setup we use to answer our research questions. We define the datasets we use and the languages we focus on in \autoref{sec:data_scope}. We select the vocabulary size in \autoref{sec:vocabulary_size}. For the experiments that require pretraining and evaluating a masked multilingual model, we explain the method in \autoref{sec:model_pretraining} and \autoref{sec:model_probing} and the same evaluation tasks. We will also use the same training and evaluation procedure. This will allow us to compare the results of different tokenization methods and answer our research questions.

% Experiments
% - data, languages - how did we select them
% - vocabulary size - what did we pick, how does it relate to the previous work
% - tokenizers replication
%     - what are the variant we create
%     - add the cluster assignment tables
%     - přidat víc podrobností o našich replikacích - jaké clustery jsme dostali (Chung, Liang), jaké vocab sizes jsme alokovali (Zheng)

% - Huggingface vs Sentencepiece
%     - what are the parameters, how do we choose them or do we test them, what are the defaults
%     - tokenizer training - memory requirements, time requirements   
% - vanilla Unigram tokenizers
% - TokMix

% - maybe a table with all the tokenizers and their explanations

% - training MLMs - technical details
%     - we train with 10K steps, 8192 batch size, 128 sequence length
%     - on 2x A100
%     - machines
%     - masked token ratio 15%

% - probing MLMs - technical details
%     - chosen tasks for each experiment, metrics for tasks
%     - utilized Huggingface examples for the downstream tasks so that there are no mistakes
    

% - training the models
% - evaluation on downstream

% To be able to compare several tokenization methods we need to fix a training and evaluation procedure that we will use throught the thesis for each experiment, be it a replication of a previous work or our own novel method. In following sections we describe the training and evaluation procedure that we use in the thesis.



% \xxx{add table with the languages with a summary of some of the properties (which languages share script, which are typologically related, which use spaces and which don't, which are low-resource)}


% For pretraining the language models, we use $\alpha = 0.3$ as suggested by \citet{conneau_unsupervised_2020-1}. For training the tokenizers, we always specify the alpha as a parameter of the tokenizer training procedure. Usually we use $\alpha = 0.0$ (1 milion lines per language, 20 milion in total) and $\alpha = 0.3$.

% We also sample uniformly from the rest of the CC100 dataset to create evaluation and test splits. Here we use $\alpha = 0.0$ to represent all languages

% - training the tokenizers
%     - the choice of implementation
%         - Huggingface vs Sentencepiece
%             - in Limisiewicz we use Huggingface
%             - in later experiments we use Sentencepiece
%                 - it has its own limitations but the Huggingface implementation seems to produce suboptimal vocabularies

%     - choice of model - BPE vs unigram
%         - all my experiments are with unigram
%         - all balancing methods use unigram so it makes sense to use unigram for the baseline
%         - there are again differences in implementation, huggingface has a better BPE
%     - the choice of coverage parameter
%         - for 120k multilingual unigram tokenizer
%             - the coverage affects the alphabet size 
%                 - max alphabet 13658, "recommended" alphabet 2678 (coverage 0.9995), 
%                 - XLM-R reproduction alphabet 8226 (0.999995), XLM-R actual alphabet 13828
%             - the coverage affects the CPT
%                 - but the difference is small +- 0.05 cpt
%                 - TODO: compute fair cpt and count unk as single characters

% As the tokenizers are the main focus of this thesis, we will first investigate how are the tokenizers influenced by various factors. For our experiments, we want to observe the influence of the choice of the tokenization method (independent variable) on the metrics we have defined and downstream tasks (dependent variables). It is therefore important to ensure that the tokenizers are trained in a consistent way for us to be able to compare between them. If we for example train the tokenizers with different vocabulary sizes, we will not be able to tell whether the differences in the metrics are caused by the choice of the tokenization method or by the choice of the vocabulary size. This is not a theoretical example as some of the methods we will be comparing do not ensure a exact final vocaublary size out of the box. A more subtle point could be made about the "character coverage" parameter. This parameter controls the size of the alphabet of the tokenizer (tokens of length 1). The alphabet size is not a parameter we are interested in, but the choice of the alphabet size influences the CPT metric. We will therefore need to ensure that the alphabet size is the same for all tokenizers.
%  We are interested in the factors such as the choice of the implementation, the amount of data needed for training, the choice of the vocabulary size, the choice of the model (BPE vs unigram) and the choice of the coverage parameter.

% \subsection{Choice of the implementation}

% In \cite{limisiewicz_tokenization_2023} we have compared the Unigram LM and BPE tokenizers. We have found that generally, the BPE tokenizer performs better on the tokenizer intrinsic metrics and the downstream tasks. During the work on the thesis we have found that this finding might have been heavily influenced by the choice of the implementation of the Unigram LM training algorithm. In \cite{limisiewicz_tokenization_2023} we have used the Huggingface implementation of the tokenizers. In the later experiments we have used the Sentencepiece implementation. We have found that the Sentencepiece implementation produces tokenizers that close the gap in the intrinsic evaluation. We have therefore decided to use the Sentencepiece implementation for all experiments in this thesis.

% \xxx{add image}

% \xxx{TODO: add the other factors - coverage (alphabet), vocab size, model}

% \section{Tokenizer implementations and important parameters}

