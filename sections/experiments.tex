
\chapter{Experimental setup}

% \textbf{Q1:} How do subword tokenizers differ in overlap and allocation of learned vocabularies?
% \textbf{Q2:} Which properties of multilingual tokenizers affect the language model representation quality?
% \textbf{Q3:} What is the reason that the standard tokenizer training method does not work well in the multilingual setting? 
% \textbf{Q4:} What is the effect of using the reproduced methods on the representation of low-resource languages? And
% \textbf{Q5:} How do the reproduced methods compare to the standard method of training the tokenizer on balanced and unbalanced joint corpus?

In this chapter, we introduce a common experimental setup we use to answer our research questions. We define the datasets we use and the languages we focus on in \autoref{sec:data_scope}. We select the vocabulary size in \autoref{sec:vocabulary_size}. For the experiments that require pretraining and evaluating a masked multilingual model, we explain the method in \autoref{sec:model_pretraining} and \autoref{sec:model_probing} and the same evaluation tasks. We will also use the same training and evaluation procedure. This will allow us to compare the results of different tokenization methods and answer our research questions.

% Experiments
% - data, languages - how did we select them
% - vocabulary size - what did we pick, how does it relate to the previous work
% - tokenizers replication
%     - what are the variant we create
%     - add the cluster assignment tables
%     - přidat víc podrobností o našich replikacích - jaké clustery jsme dostali (Chung, Liang), jaké vocab sizes jsme alokovali (Zheng)

% - Huggingface vs Sentencepiece
%     - what are the parameters, how do we choose them or do we test them, what are the defaults
%     - tokenizer training - memory requirements, time requirements   
% - vanilla Unigram tokenizers
% - TokMix

% - maybe a table with all the tokenizers and their explanations

% - training MLMs - technical details
%     - we train with 10K steps, 8192 batch size, 128 sequence length
%     - on 2x A100
%     - machines
%     - masked token ratio 15%

% - probing MLMs - technical details
%     - chosen tasks for each experiment, metrics for tasks
%     - utilized Huggingface examples for the downstream tasks so that there are no mistakes
    

% - training the models
% - evaluation on downstream

% To be able to compare several tokenization methods we need to fix a training and evaluation procedure that we will use throught the thesis for each experiment, be it a replication of a previous work or our own novel method. In following sections we describe the training and evaluation procedure that we use in the thesis.


\tomasz{I suggest to start a new chapter here: Experiments
With structure:
- Reproducing previous balanced tokenization methos;
- New tokenization method;
- Intristic Evaluation: metrics introduced in the methodology
- extrinsic Evaluation: model training, fine-tuning, evaluation tasks;
- Implementation details/ experimental setup: with all details about implemnetation. 
}


\tomasz{Remeber to motivate experiments. Which research question are they supposed to answer?
Then provide \textbf{some} details how they should answer the question.}


\section{Data and scope}
\label{sec:data_scope}

% - prepare the pretraining/tokenizer training data
%     - selecting the dataset
%         - mBERT, Zheng - wikipedias
%         - XLM, Chung, Liang - CC100 
%     - select the languages
%         - which languages to use
%         - we follow the XNLI selection more or less
%         - show the table with the languages
%     - download the data
%         - how much data to use
%     - subsample the data
%         - what alphas to use?
%             - Conneau, Chung uses 0.3 \xxx{check this}
%             - Liang uses T=2 -> alpha=0.5
%             - Zheng uses 0.7!
%         - we choose alpha 0.0 and 0.3 (for Limi it was 0.25)
%         - we also create a evaluation and test set

\begin{table}
    % uncomment the following line if you use the fitted top captions for tables
    % (see the \floatsetup[table] comments in `macros.tex`.
    %\floatbox{table}[\FBwidth]{
    \centering\footnotesize\sf
    \begin{tabular}{llrl}
    \toprule
    Language & Language code & Script \\
    \midrule
    English & en & Latin \\
    Vietnamese & vi & Latin \\
    Russian & ru & Cyrillic \\
    French & fr & Latin \\
    German & de & Latin \\
    Spanish & es & Latin \\
    Thai & th & Thai \\
    Bulgarian & bg & Cyrillic \\
    Hebrew & he & Hebrew \\
    Chinese-simplified & zh-Hans & Chinese \\
    Greek & el & Greek \\
    Turkish & tr & Latin \\
    Arabic & ar & Arabic \\
    Hindi & hi & Devanagari \\
    Tamil & ta & Tamil \\
    Georgian & ka & Georgian \\
    Urdu & ur & Arabic \\
    Telugu & te & Telugu \\
    Marathi & mr & Devanagari \\
    Swahili & sw & Latin \\
    % \addlinespace % a nice non-intrusive separator of data groups (or final table sums)
    \bottomrule
    \end{tabular}
    %}{  % uncomment if you use the \floatbox (as above), erase otherwise
    \caption{List of languages used in the experiments. \xxx{TODO: add the language sizes.} \tomasz{and language families if you have time}}
    %}  % uncomment if you use the \floatbox
    \label{tab:languages}
\end{table}

For training the vocabularies and the masked language models we follow our related work \cite{conneau_unsupervised_2020,chung_improving_2020,liang_xlm-v_2023} and use the CC100 dataset. The other common choice is using Wikipedia data \cite{devlin_bert_2019,zheng_allocating_2021} but the CC100 has been shown to improve low-resource languages such as Swahili and Urdu \cite{conneau_unsupervised_2020}.

This unlabeled, multilingual dataset was created from the Common Crawl corpus using an automatic pipeline. The data was deduplicated and language-identified. Then for each monolingual corpus the data was filtered using Kneser-Ney language models trained on Wikipedia. Documents with perplexity under a certain language-specific threshold were filtered out. The data processing pipeline is described in detail in \citet{wenzek_ccnet_nodate}. A reproduction of the dataset is available at \url{https://data.statmt.org/cc-100/}.

For the purposes of this thesis, we select 20 out of 116 languages following \citet{limisiewicz_tokenization_2023} and download 10\% of the data for each language. The reason for selecting a subset of the languages available and using only part of the data is our computational constraints. First, by limiting the amount of data, we can train the models faster. Second, by limiting the number of languages, we can scale down the vocabulary size. Because a large amount of the model parameters is in the embedding matrix, the vocabulary size has a large impact on the model size. This in turn, affects the training time and the memory requirements. We use the same \tomasz{diverse set of }20 languages for all experiments in this thesis.

The exact choice of the language subset is motivated by the downstream evaluation datasets. We use the 15 languages covered by XNLI and add 5 more. The languages are selected to cover a wide range of language families and scripts. The full list of languages is shown in \autoref{tab:languages}.

% \xxx{add table with the languages with a summary of some of the properties (which languages share script, which are typologically related, which use spaces and which don't, which are low-resource)}


% For pretraining the language models, we use $\alpha = 0.3$ as suggested by \citet{conneau_unsupervised_2020-1}. For training the tokenizers, we always specify the alpha as a parameter of the tokenizer training procedure. Usually we use $\alpha = 0.0$ (1 milion lines per language, 20 milion in total) and $\alpha = 0.3$.

% We also sample uniformly from the rest of the CC100 dataset to create evaluation and test splits. Here we use $\alpha = 0.0$ to represent all languages

% - training the tokenizers
%     - the choice of implementation
%         - Huggingface vs Sentencepiece
%             - in Limisiewicz we use Huggingface
%             - in later experiments we use Sentencepiece
%                 - it has its own limitations but the Huggingface implementation seems to produce suboptimal vocabularies

%     - choice of model - BPE vs unigram
%         - all my experiments are with unigram
%         - all balancing methods use unigram so it makes sense to use unigram for the baseline
%         - there are again differences in implementation, huggingface has a better BPE
%     - the choice of coverage parameter
%         - for 120k multilingual unigram tokenizer
%             - the coverage affects the alphabet size 
%                 - max alphabet 13658, "recommended" alphabet 2678 (coverage 0.9995), 
%                 - XLM-R reproduction alphabet 8226 (0.999995), XLM-R actual alphabet 13828
%             - the coverage affects the CPT
%                 - but the difference is small +- 0.05 cpt
%                 - TODO: compute fair cpt and count unk as single characters

% As the tokenizers are the main focus of this thesis, we will first investigate how are the tokenizers influenced by various factors. For our experiments, we want to observe the influence of the choice of the tokenization method (independent variable) on the metrics we have defined and downstream tasks (dependent variables). It is therefore important to ensure that the tokenizers are trained in a consistent way for us to be able to compare between them. If we for example train the tokenizers with different vocabulary sizes, we will not be able to tell whether the differences in the metrics are caused by the choice of the tokenization method or by the choice of the vocabulary size. This is not a theoretical example as some of the methods we will be comparing do not ensure a exact final vocaublary size out of the box. A more subtle point could be made about the "character coverage" parameter. This parameter controls the size of the alphabet of the tokenizer (tokens of length 1). The alphabet size is not a parameter we are interested in, but the choice of the alphabet size influences the CPT metric. We will therefore need to ensure that the alphabet size is the same for all tokenizers.
%  We are interested in the factors such as the choice of the implementation, the amount of data needed for training, the choice of the vocabulary size, the choice of the model (BPE vs unigram) and the choice of the coverage parameter.

% \subsection{Choice of the implementation}

% In \cite{limisiewicz_tokenization_2023} we have compared the Unigram LM and BPE tokenizers. We have found that generally, the BPE tokenizer performs better on the tokenizer intrinsic metrics and the downstream tasks. During the work on the thesis we have found that this finding might have been heavily influenced by the choice of the implementation of the Unigram LM training algorithm. In \cite{limisiewicz_tokenization_2023} we have used the Huggingface implementation of the tokenizers. In the later experiments we have used the Sentencepiece implementation. We have found that the Sentencepiece implementation produces tokenizers that close the gap in the intrinsic evaluation. We have therefore decided to use the Sentencepiece implementation for all experiments in this thesis.

% \xxx{add image}

% \xxx{TODO: add the other factors - coverage (alphabet), vocab size, model}

% \section{Tokenizer implementations and important parameters}


\section{Vocabulary size}
\label{sec:vocabulary_size}
%     - the choice of vocabulary size
%         - we know from Conneau and all balancing methods that improving performance through increasing the vocabulary size is possible
%         - therefore we know that choosing at least the same size as the vocabulary size of the balancing methods is a good idea
%         - we use 120k
%         - this was used in mBERT
%         - Chung uses 488k and 104 langs => 4.7k tokens per language
%         - Zheng uses 500k and 86 langs => 5.8k tokens per language
%         - Liang uses 900k and 104 langs => 8.7k tokens per language
%         - XLM-R uses 250k and 104 languages, we use 20 languages
%             - thus XLM-R has 2.4k tokens per language
%             - we have 6k tokens per language


We use a fixed vocabulary size of 120\,000 tokens for representing the selected 20 languages. We have chosen this vocabulary size to create a comparable setup to our related work while saving our computational resources. The related work we replicate in this thesis uses the following vocabulary sizes:

\begin{itemize}
    \item \citet{chung_improving_2020} use a vocabulary size of 488\,000 tokens for 104 languages. This amounts to 4\,692 tokens per language.
    \item \citet{zheng_allocating_2021} use a vocabulary size of 500\,000 tokens for 86 languages, which is 5\,814 tokens per language.
    \item \citet{liang_xlm-v_2023} use a vocabulary size of 901\,629 tokens for 104 languages, which is 8\,670 tokens per language.
\end{itemize}

The listed works propose to extend the vocabulary size over the previous models mBERT and XLM-R. For comparison, we also list the vocabulary size of these models:

\begin{itemize}
    \item \citet{conneau_unsupervised_2020-1} (XLM-R) use a vocabulary size of 250\,000 tokens for 104 languages, which is 2\,400 tokens per language.
    \item \citet{devlin_bert_2019} (mBERT) use a vocabulary size of 110\,000 tokens for 104 languages, which is 1\,058 tokens per language.
\end{itemize}

In our case, the vocabulary size of 120\,000 tokens representing 20 languages amounts to 6\,000 tokens per language. This is more than the vocabulary size of XLM-R and mBERT, but similar to the vocabulary size of the replicated works. 

\section{Model pretraining}
\label{sec:model_pretraining}

With the tokenizers replicated and trained, we will train the language models to be able to assess the influence of the tokenization method on the language model performance.

We use the Huggingface framework \cite{wolf_transformers_2020} for pretraining the language model. We follow \citet{conneau_unsupervised_2020} and pretrain the language model with the Masked Language Model objective. The model architecture is based on a scaled-down version of XLM-R\textsubscript{Base} \cite{conneau_unsupervised_2020}. The size of embeddings is kept at 768, the number of attention layers is reduced from 12 to 8, the number of attention heads is reduced from 12 to 6. The maximum sequence length is set to 128 tokens. The total number of parameters is roughly two times smaller than the original XLM-R\textsubscript{Base}.

The models are pretrained for 10k steps with the batch size 8192 achieved by using gradient accumulation. This amounts to $\approx 1.6$ epochs over our training dataset. The learning scheduler is linear with warmup for the first 500 steps. The learning rate is set to $5e-4$. We use the AdamW optimizer.

\section{Model probing}
\label{sec:model_probing}

To evaluate the models, we train a linear classifier on top of the model. We freeze the model parameters and train only the classifier. We use the Adam optimizer with learning rate $2e-3$ and batch size 512. We train for 60 epochs and select the best model based on the validation accuracy.

The code used for pretraining and finetuning is included in the thesis attachment.

% Model
% - mBERT - 120k vocabulary, 12-layer, 768-hidden, 12-heads, 110M parameters
