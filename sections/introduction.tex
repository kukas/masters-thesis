
\chapwithtoc{Introduction}

% Introduction should answer the following questions, ideally in this order:
% \begin{enumerate}
% \item What is the nature of the problem the thesis is addressing?
% \item What is the common approach for solving that problem now?
% \item How this thesis approaches the problem?
% \item What are the results? Did something improve?
% \item What can the reader expect in the individual chapters of the thesis?
% \end{enumerate}

% Expected length of the introduction is between 1--4 pages. Longer introductions may require sub-sectioning with appropriate headings --- use \texttt{\textbackslash{}section*} to avoid numbering (with section names like `Motivation' and `Related work'), but try to avoid lengthy discussion of anything specific. Any ``real science'' (definitions, theorems, methods, data) should go into other chapters.
% \todo{You may notice that this paragraph briefly shows different ``types'' of `quotes' in TeX, and the usage difference between a hyphen (-), en-dash (--) and em-dash (---).}


\xxx{Add an example of tokenization - in an image}

% large multilingual models
Large language models have been influential in natural language processing (NLP) field in the last few years. They have been shown to perform well on a variety of tasks and have been used as a starting point for many downstream tasks. The pretrained models are usually finetuned on a task-specific dataset and then used for the task at hand. \cite{devlin_bert_2019,radford_improving_nodate} Multilingual pretrained models extend the pretrain-finetune paradigm to multiple languages. The main advantage of these models lies in their generalization capabilities across languages. It has been shown that finetuning a multilingual model on English data makes the model capable to perform the task at hand in also the other languages it has been pre-trained on. \cite{k_cross-lingual_2022,conneau_unsupervised_2020-1}

% what is tokenization
Tokenization is the crucial first step we take when tackling a natural language processing (NLP) problem. In simple terms, tokenization converts an input text into a sequence of tokens. The tokens are then used as the input for the NLP methods. Traditionally, the term tokenization referred to the methods of splitting up an input text into words as a preprocessing step for NLP methods. After the advent of large neural models, we have seen a shift towards using subword tokenization methods such as Byte Pair Encoding (BPE) \cite{sennrich_neural_2016}, Unigram Language Model \cite{kudo_sentencepiece_2018} or Wordpiece \cite{devlin_bert_2019}. These methods split up words into subword units and therefore one word might become encoded as a series of short subwords. These subword tokens are then used as the input tokens for the neural models.

It has been shown that the choice of tokenization method has a significant effect on the performance of the NLP models - whether we talk about monolignual language models \cite{bostrom_byte_2020}, multilingual models \cite{rust_how_2021} or machine translation models \cite{kudo_sentencepiece_2018,gowda_finding_2020}. In the context of multilingual language models, the tokenization methods need to be able to handle more than a hundred languages at once. This is a challenging task, as the languages differ in their morphology, orthography and other properties. Another challenge is the fact that the languages are not equally represented in the training data. For example, \Citeauthor{rust_how_2021} have shown that the low-resource languages are under-represented in the tokenizer vocabularies. This leads to performance loss, that can be mitigated by extending the vocabulary with more language-specific tokens. \cite{rust_how_2021}

In this thesis we study the effect of tokenization on the performance of multilingual pretrained models. First we examine what properties of the tokenization are desirable for multilingual models. To this end, we propose three metrics for measuring the quality of tokenization for multilingual models. These metrics aim to measure how well is each language represented in the multilingual vocabulary. We coin this property as \textit{vocabulary allocation}. The second property we measure is the \textit{vocabulary overlap} - to what degree are the tokens shared between languages. We show that these metrics correlate with the representation quality of the model on the downstream tasks. 

% we propose simple baselines for balancing the vocabulary that perform on par with more complex methods
Then we examine and reproduce three tokenization methods \citep{chung_improving_2020,zheng_allocating_2021,liang_xlm-v_2023} proposed to tackle the problem of suboptimal tokenization in multilingual models in our setting. We compare these methods with two simpler baseline methods we propose. We measure the quality of tokenization using the metrics we proposed before. Then we select the best tokenization method and compare it with the baseline methods on downstream tasks. We show that in our setting, the baseline methods perform on par with the more complex methods proposed in the literature.

% - We show that all balancing methods improve the performance of the model on the word-level downstream tasks

\section{Contributions}


\xxx{TODO}

The work on this thesis began as a collaboration with Ing. Tomasz Limisiewicz on the paper \Citetitle{limisiewicz_tokenization_2023}. The paper was accepted to the \textit{ACL Findings 2023} and will be published in July 2023. The contributions of this paper, included in this thesis are as follows:

- Proposal of metrics for measuring the quality of tokenization for multilingual models.
- 

Additionally, the author of this thesis has continued the work 


\section{Research Questions}

- (Q1) How do sub-word tokenizers differ in overlap and allocation of learned vocabularies?
- (Q2) Which properties of multilingual tokenizers affect the LMâ€™s representation quality?


% summary of the thesis structure with links to the chapters
The thesis is structured as follows: In Chapter 1 we provide the necessary background on tokenization and multilingual models. 

