
\chapter{Introduction}

% Introduction should answer the following questions, ideally in this order:
% \begin{enumerate}
% \item What is the nature of the problem the thesis is addressing?
% \item What is the common approach for solving that problem now?
% \item How this thesis approaches the problem?
% \item What are the results? Did something improve?
% \item What can the reader expect in the individual chapters of the thesis?
% \end{enumerate}

% Expected length of the introduction is between 1--4 pages. Longer introductions may require sub-sectioning with appropriate headings --- use \texttt{\textbackslash{}section*} to avoid numbering (with section names like `Motivation' and `Related work'), but try to avoid lengthy discussion of anything specific. Any ``real science'' (definitions, theorems, methods, data) should go into other chapters.
% \todo{You may notice that this paragraph briefly shows different ``types'' of `quotes' in TeX, and the usage difference between a hyphen (-), en-dash (--) and em-dash (---).}


\xxx{Image: text in three languages, tokenized text in three languages, multilingual model, classification head, outputs for each example}

\tomasz{It's my comment.}
% large multilingual models
Large neural language models have been shown to perform well on a variety of Natural Language Processing (NLP) tasks and have become a de-facto standard for tackling tasks that require language understanding. The main advantage of these pretrained models is that they are able to leverage a large, unannotated pretraining corpus. After pretraining, we usually need only a fraction of annotated, task-specific data for finetuning the model for the task at hand. \cite{devlin_bert_2019,radford_improving_nodate} \tomasz{I don't like the last sentence -> ... to perform well in other/less resourcefull languages.} Multilingual pretrained models extend the pretrain-finetune paradigm to multiple languages. By pretraining on large, \textit{multilingual} corpus, these models provide high quality representations even for low-resource languages where a dedicated monolingual model might not even exist. Furthermore, it has been shown that finetuning a multilingual model with data from a high-resource language makes the model capable to perform the task at hand in the other languages it has been pre-trained on. This "cross-lingual transfer", a phenomenon specific to multilingual models, allows the usage of the model for languages where task-specific data is not available. \cite{k_cross-lingual_2022,conneau_unsupervised_2020-1}

Even though multilingual models are capable of narrowing the gap between high-resource languages and low-resource languages, they still suffer from the fact that the languages are not equally represented in the pretraining data. This leads to a lower performance on the low-resource languages. \cite{conneau_unsupervised_2020} Furthermore, \Citeauthor{conneau_unsupervised_2020} has shown that increasing the number of languages while keeping the model size fixed leads to a decrease in performance across all languages, tradeoff referred to as the \textit{curse of multilinguality}. \cite{conneau_unsupervised_2020} \tomasz{I don't like this term (CoM), IMO it's overused and vague.}

% what is tokenization
Tokenization is the crucial first step we take when tackling a NLP problem. In simple terms, tokenization converts an input text into a sequence of tokens. The tokens are then used as the input for the NLP methods. Traditionally, the term tokenization referred to the methods of splitting up an input text into words as a preprocessing step for NLP methods. After\tomasz{With ...} the advent of large neural models, we have seen a shift towards using subword tokenization methods such as Byte Pair Encoding (BPE) \cite{sennrich_neural_2016}, Unigram Language Model \cite{kudo_sentencepiece_2018} or Wordpiece \cite{devlin_bert_2019}. \tomasz{Use logical connector here, subword tokenizers used because ...}These methods split up words into subword units and therefore one word might become encoded as a series of short subwords. These subword tokens are then used as the input tokens for the neural models. \xxx{maybe do not specify the tokenization methods here. Dedicate the space to connecting the tokenization with the multilingual models.} \tomasz{I agree with the first suggestion..}

It has been shown that the choice of tokenization method has a significant effect on the performance of the NLP models - whether we talk about monolingual language models \cite{bostrom_byte_2020}, multilingual models \cite{rust_how_2021} or machine translation models \cite{kudo_sentencepiece_2018,gowda_finding_2020}. In the context of multilingual language models, the tokenization methods need to be able to \tomasz{getr rid of "need to be able"} handle more than a hundred languages at once. This is a challenging task, as the languages differ in their morphology, orthography and other properties.\tomasz{script}} Another challenge is, as mentioned before, the fact that the languages are not equally represented in the training data. \Citeauthor{rust_how_2021} have shown that the low-resource languages are under-represented in the tokenizer vocabularies. This leads to performance loss, that can be mitigated by changing the vocabulary to one with more language-specific tokens. \cite{rust_how_2021}

\xxx{in the thesis annotation, we say that we look for an optimal tokenization method for multilingual language models. This of course depends on how we define optimal. I think I should connect the focus on low-resource languages with the definition of optimal.}

\xxx{rewrite the introduction so that the research question are a natural consequence of the introduction - the reader should be able to guess the research questions from the introduction}

In this thesis we study the effect of tokenization on the performance of multilingual pretrained models. \xxx{why; We propose metrics to be able to assess the differences between tokenizers before costly pretrainig. We can use these metrics to compare between methods, see the differences in what they do and choosing the best option for our usecase} First we examine what properties of the tokenizers are desirable for multilingual models with respect to low- and high- resource languages. To this end, we propose three metrics --- Characters per Token, Average Rank and Jensen-Shannon Divergence between token distributions --- for measuring the quality of tokenization across the target languages. First two metrics aim to measure how well is each language represented in the multilingual vocabulary. We coin this property as \textit{vocabulary allocation}. The third metric measures the \textit{vocabulary overlap} --- to what degree are the tokens shared between languages. We show that these metrics correlate with the representation quality of the model on the downstream tasks.\xxx{why is the correlation needed?} \xxx{maybe motivate more why we need these metrics}

% we propose simple baselines for balancing the vocabulary that perform on par with more complex methods
Then we examine and reproduce three tokenization methods \citep{chung_improving_2020,zheng_allocating_2021,liang_xlm-v_2023} proposed to tackle the problem of suboptimal tokenization and underrepresentation of low-resource languages in multilingual models. \xxx{before this sentence: why do we need the balancing methods} In our scaled-down setting, we compare these methods with two simpler baseline methods we propose. The first baseline method \xxx{name} is simply to sample the languages uniformly during the tokenizer training. The second method \xxx{name} takes the balancing one step further and resamples the word distributions in hopes of leveling a hypothesized topic imbalance across the languages. 
Gathering all five tokenization methods together, we then measure their \textit{vocabulary allocation} and \textit{vocabulary overlap} using the metrics we proposed and validated before. We select the best tokenization method and compare it with our baseline methods on downstream tasks. We show that in our setting, the baseline methods perform on par with the more complex methods proposed in the literature. \xxx{we posit that the tokenization is a zero-sum game and by clustering or training separate tokenizers for each language, we only shift the performance from one language to another.}
\tomasz{I don't think you can say that. If you have really bad tokenizer it's not a zero-sum game.}

% - We show that all balancing methods improve the performance of the model on the word-level downstream tasks

\section{Contributions}

The work on this thesis began as a collaboration with Ing. Tomasz Limisiewicz on the paper \Citetitle{limisiewicz_tokenization_2023}. The paper was accepted to the \textit{ACL Findings 2023} and will be published in July 2023. The contributions of this paper, included in this thesis are as follows:

\begin{itemize}
    \item Proposal of metrics for measuring the quality of tokenization for multilingual models.
    \item Validation of the metrics on a multitude of downstream tasks using four different tokenization methods.
\end{itemize}

Additionally, the thesis continues in the multilingual tokenization research by:

\begin{itemize}
    \item Investigating the effect of important choices in the tokenization process on the quality of the tokenization.
    \item Reproducing three tokenization methods proposed in the literature.
    \item Proposing two simpler, alternative methods for improving the tokenization for low-resource languages.
    \item Comparing the tokenization methods on downstream tasks and showing that the simpler methods perform on par with the more complex methods.
\end{itemize}

\section{Research Questions}


- (Q1) How do sub-word tokenizers differ in overlap and allocation of learned vocabularies?
- (Q2) Which properties of multilingual tokenizers affect the LMâ€™s representation quality?

\xxx{TODO}


% summary of the thesis structure with links to the chapters
The thesis is structured as follows: In Chapter 1 we provide the necessary background on tokenization and multilingual models.  \xxx{TODO}

% from methodology:

% The main goal of the thesis is to \textbf{improve multilingual models with better tokenization}. The standard procedure followed by most of the large multilingual models is to train the tokenizer on the joint, multilingual corpus used for training the model \cite{mielke_between_2021,conneau_unsupervised_2020,conneau_cross-lingual_nodate}. As many authors pointed out \cite{wang_improving_2019,chung_improving_2020,rust_how_2021}, by using this simple approach the resulting tokenizer overtokenizes some of the low-resource languages, especially those that do not use the latin script. The latin script tokens appear more often in the resulting vocabulary because the procedure of merging all corpora together greatly increases the occurence statistics of the Latin character n-grams. \cite{zheng_allocating_2021}

% To this end, several methods of mitigating this issue were suggested. \citet{chung_improving_2020} proposed a method based on creating multiple clusters of corpora instead of one joint cluster. The multilingual tokenizer is then created by merging together smaller, specialized tokenizers trained on these clusters. \citet{zheng_allocating_2021} proposed a method of allocating the vocabulary budget to the languages based on a heuristic function similar to entropy. \citet{liang_xlm-v_2023} showed that by combining the two approaches, the resulting tokenizer scales better to larger vocabulary sizes.

% Unrelated to multilingual models, we have seen also other approaches of tokenizer improvement related to Domain Adaptation. In the paper \Citetitle{gururangan_dont_2020}, \citeauthor{gururangan_dont_2020} \cite{gururangan_dont_2020} showed that a continued pretraining models on a domain-specific data leads to performance improvements of in-domain tasks. \citet{sachidananda_efficient_2021} showed that these improvements can be achieved not only by costly pretraining continuation but also by simply extending the model vocabulary with a small amount of domain-specific tokens that are then fine-tuned on the domain tasks.

% \xxx{rewrite this paragraph}
% In this thesis we first replicate the results of \citet{chung_improving_2020}, \citet{zheng_allocating_2021} and \citet{liang_xlm-v_2023} on our selected subset of languages. We then use our evaluation framework of \citet{limisiewicz_tokenization_2023} to compare these methods and pick a strong baseline. We then present our novel method of improving the tokenizer based on a extension of the clustering idea of \citet{chung_improving_2020}.
