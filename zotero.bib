
@misc{liang_xlm-v_2023,
	title = {{XLM}-{V}: {Overcoming} the {Vocabulary} {Bottleneck} in {Multilingual} {Masked} {Language} {Models}},
	shorttitle = {{XLM}-{V}},
	url = {http://arxiv.org/abs/2301.10472},
	abstract = {Large multilingual language models typically rely on a single vocabulary shared across 100+ languages. As these models have increased in parameter count and depth, vocabulary size has remained largely unchanged. This vocabulary bottleneck limits the representational capabilities of multilingual models like XLM-R. In this paper, we introduce a new approach for scaling to very large multilingual vocabularies by de-emphasizing token sharing between languages with little lexical overlap and assigning vocabulary capacity to achieve sufﬁcient coverage for each individual language. Tokenizations using our vocabulary are typically more semantically meaningful and shorter compared to XLM-R. Leveraging this improved vocabulary, we train XLM-V, a multilingual language model with a one million token vocabulary. XLM-V outperforms XLMR on every task we tested on ranging from natural language inference (XNLI), question answering (MLQA, XQuAD, TyDiQA), and named entity recognition (WikiAnn) to lowresource tasks (Americas NLI, MasakhaNER).},
	language = {en},
	urldate = {2023-02-01},
	publisher = {arXiv},
	author = {Liang, Davis and Gonen, Hila and Mao, Yuning and Hou, Rui and Goyal, Naman and Ghazvininejad, Marjan and Zettlemoyer, Luke and Khabsa, Madian},
	month = jan,
	year = {2023},
	note = {arXiv:2301.10472 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, tokenizers, method},
	file = {Liang et al. - 2023 - XLM-V Overcoming the Vocabulary Bottleneck in Mul.pdf:/home/jirka/Zotero/storage/7Y6YHPGP/Liang et al. - 2023 - XLM-V Overcoming the Vocabulary Bottleneck in Mul.pdf:application/pdf},
}

@inproceedings{zheng_allocating_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Allocating {Large} {Vocabulary} {Capacity} for {Cross}-{Lingual} {Language} {Model} {Pre}-{Training}},
	url = {https://aclanthology.org/2021.emnlp-main.257},
	doi = {10.18653/v1/2021.emnlp-main.257},
	language = {en},
	urldate = {2023-02-02},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Zheng, Bo and Dong, Li and Huang, Shaohan and Singhal, Saksham and Che, Wanxiang and Liu, Ting and Song, Xia and Wei, Furu},
	year = {2021},
	keywords = {tokenizers, method},
	pages = {3203--3215},
	file = {Zheng et al. - 2021 - Allocating Large Vocabulary Capacity for Cross-Lin.pdf:/home/jirka/Zotero/storage/QVF5RXAN/Zheng et al. - 2021 - Allocating Large Vocabulary Capacity for Cross-Lin.pdf:application/pdf},
}

@article{devlin_bert_nodate,
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pretrain deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be ﬁnetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial taskspeciﬁc architecture modiﬁcations.},
	language = {en},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	file = {Devlin et al. - BERT Pre-training of Deep Bidirectional Transform.pdf:/home/jirka/Zotero/storage/V2YHXIXJ/Devlin et al. - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf},
}

@inproceedings{conneau_unsupervised_2020,
	address = {Online},
	title = {Unsupervised {Cross}-lingual {Representation} {Learning} at {Scale}},
	url = {https://aclanthology.org/2020.acl-main.747},
	doi = {10.18653/v1/2020.acl-main.747},
	abstract = {This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6\% average accuracy on XNLI, +13\% average F1 score on MLQA, and +2.4\% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7\% in XNLI accuracy for Swahili and 11.4\% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.},
	urldate = {2023-02-04},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzmán, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
	month = jul,
	year = {2020},
	pages = {8440--8451},
	file = {Full Text PDF:/home/jirka/Zotero/storage/NPNEUKIL/Conneau et al. - 2020 - Unsupervised Cross-lingual Representation Learning.pdf:application/pdf},
}

@inproceedings{conneau_unsupervised_2020-1,
	address = {Online},
	title = {Unsupervised {Cross}-lingual {Representation} {Learning} at {Scale}},
	url = {https://aclanthology.org/2020.acl-main.747},
	doi = {10.18653/v1/2020.acl-main.747},
	abstract = {This paper shows that pretraining multilingual language models at scale leads to significant performance gains for a wide range of cross-lingual transfer tasks. We train a Transformer-based masked language model on one hundred languages, using more than two terabytes of filtered CommonCrawl data. Our model, dubbed XLM-R, significantly outperforms multilingual BERT (mBERT) on a variety of cross-lingual benchmarks, including +14.6\% average accuracy on XNLI, +13\% average F1 score on MLQA, and +2.4\% F1 score on NER. XLM-R performs particularly well on low-resource languages, improving 15.7\% in XNLI accuracy for Swahili and 11.4\% for Urdu over previous XLM models. We also present a detailed empirical analysis of the key factors that are required to achieve these gains, including the trade-offs between (1) positive transfer and capacity dilution and (2) the performance of high and low resource languages at scale. Finally, we show, for the first time, the possibility of multilingual modeling without sacrificing per-language performance; XLM-R is very competitive with strong monolingual models on the GLUE and XNLI benchmarks. We will make our code and models publicly available.},
	urldate = {2023-02-04},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Conneau, Alexis and Khandelwal, Kartikay and Goyal, Naman and Chaudhary, Vishrav and Wenzek, Guillaume and Guzmán, Francisco and Grave, Edouard and Ott, Myle and Zettlemoyer, Luke and Stoyanov, Veselin},
	month = jul,
	year = {2020},
	keywords = {model, method},
	pages = {8440--8451},
	file = {Full Text PDF:/home/jirka/Zotero/storage/5AXQ7DJH/Conneau et al. - 2020 - Unsupervised Cross-lingual Representation Learning.pdf:application/pdf},
}

@inproceedings{conneau_xnli_2018,
	address = {Brussels, Belgium},
	title = {{XNLI}: {Evaluating} {Cross}-lingual {Sentence} {Representations}},
	shorttitle = {{XNLI}},
	url = {https://aclanthology.org/D18-1269},
	doi = {10.18653/v1/D18-1269},
	abstract = {State-of-the-art natural language processing systems rely on supervision in the form of annotated data to learn competent models. These models are generally trained on data in a single language (usually English), and cannot be directly used beyond that language. Since collecting data in every language is not realistic, there has been a growing interest in cross-lingual language understanding (XLU) and low-resource cross-language transfer. In this work, we construct an evaluation set for XLU by extending the development and test sets of the Multi-Genre Natural Language Inference Corpus (MultiNLI) to 14 languages, including low-resource languages such as Swahili and Urdu. We hope that our dataset, dubbed XNLI, will catalyze research in cross-lingual sentence understanding by providing an informative standard evaluation task. In addition, we provide several baselines for multilingual sentence understanding, including two based on machine translation systems, and two that use parallel data to train aligned multilingual bag-of-words and LSTM encoders. We find that XNLI represents a practical and challenging evaluation suite, and that directly translating the test data yields the best performance among available baselines.},
	urldate = {2023-02-04},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Conneau, Alexis and Rinott, Ruty and Lample, Guillaume and Williams, Adina and Bowman, Samuel and Schwenk, Holger and Stoyanov, Veselin},
	month = oct,
	year = {2018},
	keywords = {evaluation},
	pages = {2475--2485},
	file = {Full Text PDF:/home/jirka/Zotero/storage/TTAV7HMF/Conneau et al. - 2018 - XNLI Evaluating Cross-lingual Sentence Representa.pdf:application/pdf},
}

@misc{lample_cross-lingual_2019,
	title = {Cross-lingual {Language} {Model} {Pretraining}},
	url = {http://arxiv.org/abs/1901.07291},
	abstract = {Recent studies have demonstrated the efﬁciency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classiﬁcation, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9\% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT’16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT’16 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available.},
	language = {en},
	urldate = {2023-02-04},
	publisher = {arXiv},
	author = {Lample, Guillaume and Conneau, Alexis},
	month = jan,
	year = {2019},
	note = {arXiv:1901.07291 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{mielke_between_2021,
	title = {Between words and characters: {A} {Brief} {History} of {Open}-{Vocabulary} {Modeling} and {Tokenization} in {NLP}},
	shorttitle = {Between words and characters},
	url = {https://www.semanticscholar.org/paper/Between-words-and-characters%3A-A-Brief-History-of-in-Mielke-Alyafeai/d617f51833860dc50d202af7f80be71304b2e994},
	abstract = {What are the units of text that we want to model? From bytes to multi-word expressions, text can be analyzed and generated at many granularities. Until recently, most natural language processing (NLP) models operated over words, treating those as discrete and atomic tokens, but starting with byte-pair encoding (BPE), subword-based approaches have become dominant in many areas, enabling small vocabularies while still allowing for fast inference. Is the end of the road character-level model or byte-level processing? In this survey, we connect several lines of work from the pre-neural and neural era, by showing how hybrid approaches of words and characters as well as subwordbased approaches based on learned segmentation have been proposed and evaluated. We conclude that there is and likely will never be a silver bullet singular solution for all applications and that thinking seriously about tokenization remains important for many applications.},
	urldate = {2023-02-05},
	journal = {ArXiv},
	author = {Mielke, Sabrina J. and Alyafeai, Zaid and Salesky, Elizabeth and Raffel, Colin and Dey, Manan and Gallé, Matthias and Raja, Arun and Si, Chenglei and Lee, Wilson Y. and Sagot, Benoît and Tan, Samson},
	month = dec,
	year = {2021},
	file = {Full Text PDF:/home/jirka/Zotero/storage/KGVQCELP/Mielke et al. - 2021 - Between words and characters A Brief History of O.pdf:application/pdf},
}

@misc{mielke_between_2021-1,
	title = {Between words and characters: {A} {Brief} {History} of {Open}-{Vocabulary} {Modeling} and {Tokenization} in {NLP}},
	shorttitle = {Between words and characters},
	url = {http://arxiv.org/abs/2112.10508},
	abstract = {What are the units of text that we want to model? From bytes to multi-word expressions, text can be analyzed and generated at many granularities. Until recently, most natural language processing (NLP) models operated over words, treating those as discrete and atomic tokens, but starting with byte-pair encoding (BPE), subword-based approaches have become dominant in many areas, enabling small vocabularies while still allowing for fast inference. Is the end of the road character-level model or byte-level processing? In this survey, we connect several lines of work from the pre-neural and neural era, by showing how hybrid approaches of words and characters as well as subwordbased approaches based on learned segmentation have been proposed and evaluated. We conclude that there is and likely will never be a silver bullet singular solution for all applications and that thinking seriously about tokenization remains important for many applications.},
	language = {en},
	urldate = {2023-02-05},
	publisher = {arXiv},
	author = {Mielke, Sabrina J. and Alyafeai, Zaid and Salesky, Elizabeth and Raffel, Colin and Dey, Manan and Gallé, Matthias and Raja, Arun and Si, Chenglei and Lee, Wilson Y. and Sagot, Benoît and Tan, Samson},
	month = dec,
	year = {2021},
	note = {arXiv:2112.10508 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {Mielke et al. - 2021 - Between words and characters A Brief History of O.pdf:/home/jirka/Zotero/storage/6J8Y2JAR/Mielke et al. - 2021 - Between words and characters A Brief History of O.pdf:application/pdf},
}

@article{chan_latent_2017,
	title = {{LATENT} {SEQUENCE} {DECOMPOSITIONS}},
	abstract = {Sequence-to-sequence models rely on a ﬁxed decomposition of the target sequences into a sequence of tokens that may be words, word-pieces or characters. The choice of these tokens and the decomposition of the target sequences into a sequence of tokens is often static, and independent of the input, output data domains. This can potentially lead to a sub-optimal choice of token dictionaries, as the decomposition is not informed by the particular problem being solved. In this paper we present Latent Sequence Decompositions (LSD), a framework in which the decomposition of sequences into constituent tokens is learnt during the training of the model. The decomposition depends both on the input sequence and on the output sequence. In LSD, during training, the model samples decompositions incrementally, from left to right by locally sampling between valid extensions. We experiment with the Wall Street Journal speech recognition task. Our LSD model achieves 12.9\% WER compared to a character baseline of 14.8\% WER. When combined with a convolutional network on the encoder, we achieve a WER of 9.6\%.},
	language = {en},
	author = {Chan, William and Zhang, Yu and Le, Quoc V and Jaitly, Navdeep},
	year = {2017},
	file = {Chan et al. - 2017 - LATENT SEQUENCE DECOMPOSITIONS.pdf:/home/jirka/Zotero/storage/HWKYBDN9/Chan et al. - 2017 - LATENT SEQUENCE DECOMPOSITIONS.pdf:application/pdf},
}

@inproceedings{chan_latent_2022,
	title = {Latent {Sequence} {Decompositions}},
	url = {https://openreview.net/forum?id=SyQq185lg},
	abstract = {Sequence-to-sequence models rely on a fixed decomposition of the target sequences into a sequence of tokens that may be words, word-pieces or characters. The choice of these tokens and the decomposition of the target sequences into a sequence of tokens is often static, and independent of the input, output data domains. This can potentially lead to a sub-optimal choice of token dictionaries, as the decomposition is not informed by the particular problem being solved. In this paper we present Latent Sequence Decompositions (LSD), a framework in which the decomposition of sequences into constituent tokens is learnt during the training of the model. The decomposition depends both on the input sequence and on the output sequence. In LSD, during training, the model samples decompositions incrementally, from left to right by locally sampling between valid extensions. We experiment with the Wall Street Journal speech recognition task. Our LSD model achieves 12.9\% WER compared to a character baseline of 14.8\% WER. When combined with a convolutional network on the encoder, we achieve a WER of 9.6\%.},
	language = {en},
	urldate = {2023-02-06},
	author = {Chan, William and Zhang, Yu and Le, Quoc and Jaitly, Navdeep},
	month = jul,
	year = {2022},
	file = {Full Text PDF:/home/jirka/Zotero/storage/B9CGBJAC/Chan et al. - 2022 - Latent Sequence Decompositions.pdf:application/pdf},
}

@inproceedings{he_dynamic_2020,
	address = {Online},
	title = {Dynamic {Programming} {Encoding} for {Subword} {Segmentation} in {Neural} {Machine} {Translation}},
	url = {https://aclanthology.org/2020.acl-main.275},
	doi = {10.18653/v1/2020.acl-main.275},
	abstract = {This paper introduces Dynamic Programming Encoding (DPE), a new segmentation algorithm for tokenizing sentences into subword units. We view the subword segmentation of output sentences as a latent variable that should be marginalized out for learning and inference. A mixed character-subword transformer is proposed, which enables exact log marginal likelihood estimation and exact MAP inference to find target segmentations with maximum posterior probability. DPE uses a lightweight mixed character-subword transformer as a means of pre-processing parallel data to segment output sentences using dynamic programming. Empirical results on machine translation suggest that DPE is effective for segmenting output sentences and can be combined with BPE dropout for stochastic segmentation of source sentences. DPE achieves an average improvement of 0.9 BLEU over BPE (Sennrich et al., 2016) and an average improvement of 0.55 BLEU over BPE dropout (Provilkov et al., 2019) on several WMT datasets including English {\textbackslash}textless={\textbackslash}textgreater (German, Romanian, Estonian, Finnish, Hungarian).},
	urldate = {2023-02-06},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {He, Xuanli and Haffari, Gholamreza and Norouzi, Mohammad},
	month = jul,
	year = {2020},
	keywords = {tokenizers, method},
	pages = {3042--3051},
	file = {Full Text PDF:/home/jirka/Zotero/storage/T8C227U9/He et al. - 2020 - Dynamic Programming Encoding for Subword Segmentat.pdf:application/pdf},
}

@inproceedings{provilkov_bpe-dropout_2020,
	address = {Online},
	title = {{BPE}-{Dropout}: {Simple} and {Effective} {Subword} {Regularization}},
	shorttitle = {{BPE}-{Dropout}},
	url = {https://aclanthology.org/2020.acl-main.170},
	doi = {10.18653/v1/2020.acl-main.170},
	abstract = {Subword segmentation is widely used to address the open vocabulary problem in machine translation. The dominant approach to subword segmentation is Byte Pair Encoding (BPE), which keeps the most frequent words intact while splitting the rare ones into multiple tokens. While multiple segmentations are possible even with the same vocabulary, BPE splits words into unique sequences; this may prevent a model from better learning the compositionality of words and being robust to segmentation errors. So far, the only way to overcome this BPE imperfection, its deterministic nature, was to create another subword segmentation algorithm (Kudo, 2018). In contrast, we show that BPE itself incorporates the ability to produce multiple segmentations of the same word. We introduce BPE-dropout - simple and effective subword regularization method based on and compatible with conventional BPE. It stochastically corrupts the segmentation procedure of BPE, which leads to producing multiple segmentations within the same fixed BPE framework. Using BPE-dropout during training and the standard BPE during inference improves translation quality up to 2.3 BLEU compared to BPE and up to 0.9 BLEU compared to the previous subword regularization.},
	urldate = {2023-02-08},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Provilkov, Ivan and Emelianenko, Dmitrii and Voita, Elena},
	month = jul,
	year = {2020},
	keywords = {tokenizers, method},
	pages = {1882--1892},
	file = {Full Text PDF:/home/jirka/Zotero/storage/NYMM9JAU/Provilkov et al. - 2020 - BPE-Dropout Simple and Effective Subword Regulari.pdf:application/pdf},
}

@inproceedings{signoroni_hft_2022,
	title = {{HFT}: {High} {Frequency} {Tokens} for {Low}-{Resource} {NMT}},
	shorttitle = {{HFT}},
	url = {https://www.semanticscholar.org/paper/HFT%3A-High-Frequency-Tokens-for-Low-Resource-NMT-Signoroni-Rychl%C3%BD/4ddd463c22fab24e6d89105b6aa887e149832dac},
	abstract = {Tokenization has been shown to impact the quality of downstream tasks, such as Neural Machine Translation (NMT), which is susceptible to out-of-vocabulary words and low frequency training data. Current state-of-the-art algorithms have been helpful in addressing the issues of out-of-vocabulary words, bigger vocabulary sizes and token frequency by implementing subword segmentation. We argue, however, that there is still room for improvement, in particular regarding low-frequency tokens in the training data. In this paper, we present “High Frequency Tokenizer”, or HFT, a new language-independent subword segmentation algorithm that addresses this issue. We also propose a new metric to measure the frequency coverage of a tokenizer’s vocabulary, based on a frequency rank weighted average of the frequency values of its items. We experiment with a diverse set of language corpora, vocabulary sizes, and writing systems and report improvements on both frequency statistics and on the average length of the output. We also observe a positive impact on downstream NMT.},
	urldate = {2023-02-10},
	author = {Signoroni, Edoardo and Rychlý, P.},
	year = {2022},
	file = {Full Text PDF:/home/jirka/Zotero/storage/3SJS85DN/Signoroni and Rychlý - 2022 - HFT High Frequency Tokens for Low-Resource NMT.pdf:application/pdf},
}

@inproceedings{kudo_sentencepiece_2018,
	address = {Brussels, Belgium},
	title = {{SentencePiece}: {A} simple and language independent subword tokenizer and detokenizer for {Neural} {Text} {Processing}},
	shorttitle = {{SentencePiece}},
	url = {https://aclanthology.org/D18-2012},
	doi = {10.18653/v1/D18-2012},
	abstract = {This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at https://github.com/google/sentencepiece.},
	urldate = {2023-02-14},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Kudo, Taku and Richardson, John},
	month = nov,
	year = {2018},
	pages = {66--71},
	file = {Full Text PDF:/home/jirka/Zotero/storage/LY8GL742/Kudo and Richardson - 2018 - SentencePiece A simple and language independent s.pdf:application/pdf},
}

@misc{yehezkel_incorporating_2023,
	title = {Incorporating {Context} into {Subword} {Vocabularies}},
	url = {http://arxiv.org/abs/2210.07095},
	abstract = {Most current popular subword tokenizers are trained based on word frequency statistics over a corpus, without considering information about co-occurrence or context. Nevertheless, the resulting vocabularies are used in language models’ highly contextualized settings. We present SAGE, a tokenizer that tailors subwords for their downstream use by baking in the contextualized signal at the vocabulary creation phase. We show that SAGE does a better job than current widespread tokenizers in keeping token contexts cohesive, while not incurring a large price in terms of encoding efﬁciency or domain robustness. SAGE improves performance on English GLUE classiﬁcation tasks as well as on NER, and on Inference and NER in Turkish, demonstrating its robustness to language properties such as morphological exponence and agglutination.},
	language = {en},
	urldate = {2023-02-17},
	publisher = {arXiv},
	author = {Yehezkel, Shaked and Pinter, Yuval},
	month = feb,
	year = {2023},
	note = {arXiv:2210.07095 [cs]},
	keywords = {Computer Science - Computation and Language, tokenizers, method},
	file = {Yehezkel and Pinter - 2023 - Incorporating Context into Subword Vocabularies.pdf:/home/jirka/Zotero/storage/7N6GX5Q7/Yehezkel and Pinter - 2023 - Incorporating Context into Subword Vocabularies.pdf:application/pdf},
}

@misc{mikolov_advances_2017,
	title = {Advances in {Pre}-{Training} {Distributed} {Word} {Representations}},
	url = {http://arxiv.org/abs/1712.09405},
	abstract = {Many Natural Language Processing applications nowadays rely on pre-trained word representations estimated from large text corpora such as news collections, Wikipedia and Web Crawl. In this paper, we show how to train high-quality word vector representations by using a combination of known tricks that are however rarely used together. The main result of our work is the new set of publicly available pre-trained models that outperform the current state of the art by a large margin on a number of tasks.},
	language = {en},
	urldate = {2023-02-21},
	publisher = {arXiv},
	author = {Mikolov, Tomas and Grave, Edouard and Bojanowski, Piotr and Puhrsch, Christian and Joulin, Armand},
	month = dec,
	year = {2017},
	note = {arXiv:1712.09405 [cs]},
	keywords = {Computer Science - Computation and Language, model},
	file = {Mikolov et al. - 2017 - Advances in Pre-Training Distributed Word Represen.pdf:/home/jirka/Zotero/storage/568RFIKG/Mikolov et al. - 2017 - Advances in Pre-Training Distributed Word Represen.pdf:application/pdf},
}

@article{bojanowski_enriching_2017,
	title = {Enriching {Word} {Vectors} with {Subword} {Information}},
	volume = {5},
	issn = {2307-387X},
	url = {https://direct.mit.edu/tacl/article/43387},
	doi = {10.1162/tacl_a_00051},
	abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character n-grams. A vector representation is associated to each character n-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
	language = {en},
	urldate = {2023-02-21},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
	month = dec,
	year = {2017},
	keywords = {model},
	pages = {135--146},
	file = {Bojanowski et al. - 2017 - Enriching Word Vectors with Subword Information.pdf:/home/jirka/Zotero/storage/XB3I5A25/Bojanowski et al. - 2017 - Enriching Word Vectors with Subword Information.pdf:application/pdf},
}

@inproceedings{bostrom_byte_2020,
	address = {Online},
	title = {Byte {Pair} {Encoding} is {Suboptimal} for {Language} {Model} {Pretraining}},
	url = {https://aclanthology.org/2020.findings-emnlp.414},
	doi = {10.18653/v1/2020.findings-emnlp.414},
	abstract = {The success of pretrained transformer language models (LMs) in natural language processing has led to a wide range of pretraining setups. In particular, these models employ a variety of subword tokenization methods, most notably byte-pair encoding (BPE) (Sennrich et al., 2016; Gage, 1994), the WordPiece method (Schuster and Nakajima, 2012), and unigram language modeling (Kudo, 2018), to segment text. However, to the best of our knowledge, the literature does not contain a direct evaluation of the impact of tokenization on language model pretraining. We analyze differences between BPE and unigram LM tokenization, finding that the latter method recovers subword units that align more closely with morphology and avoids problems stemming from BPE's greedy construction procedure. We then compare the fine-tuned task performance of identical transformer masked language models pretrained with these tokenizations. Across downstream tasks and two languages (English and Japanese), we find that the unigram LM tokenization method matches or outperforms BPE. We hope that developers of future pretrained LMs will consider adopting the unigram LM method over the more prevalent BPE.},
	urldate = {2023-02-28},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Bostrom, Kaj and Durrett, Greg},
	month = nov,
	year = {2020},
	keywords = {tokenizers, analysis},
	pages = {4617--4624},
	file = {Full Text PDF:/home/jirka/Zotero/storage/BQBASEYN/Bostrom and Durrett - 2020 - Byte Pair Encoding is Suboptimal for Language Mode.pdf:application/pdf},
}

@inproceedings{kudo_subword_2018,
	address = {Melbourne, Australia},
	title = {Subword {Regularization}: {Improving} {Neural} {Network} {Translation} {Models} with {Multiple} {Subword} {Candidates}},
	shorttitle = {Subword {Regularization}},
	url = {https://aclanthology.org/P18-1007},
	doi = {10.18653/v1/P18-1007},
	abstract = {Subword units are an effective way to alleviate the open vocabulary problems in neural machine translation (NMT). While sentences are usually converted into unique subword sequences, subword segmentation is potentially ambiguous and multiple segmentations are possible even with the same vocabulary. The question addressed in this paper is whether it is possible to harness the segmentation ambiguity as a noise to improve the robustness of NMT. We present a simple regularization method, subword regularization, which trains the model with multiple subword segmentations probabilistically sampled during training. In addition, for better subword sampling, we propose a new subword segmentation algorithm based on a unigram language model. We experiment with multiple corpora and report consistent improvements especially on low resource and out-of-domain settings.},
	urldate = {2023-03-01},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Kudo, Taku},
	month = jul,
	year = {2018},
	keywords = {tokenizers, method},
	pages = {66--75},
	file = {Full Text PDF:/home/jirka/Zotero/storage/HIQBM4F7/Kudo - 2018 - Subword Regularization Improving Neural Network T.pdf:application/pdf},
}

@inproceedings{rust_how_2021,
	address = {Online},
	title = {How {Good} is {Your} {Tokenizer}? {On} the {Monolingual} {Performance} of {Multilingual} {Language} {Models}},
	shorttitle = {How {Good} is {Your} {Tokenizer}?},
	url = {https://aclanthology.org/2021.acl-long.243},
	doi = {10.18653/v1/2021.acl-long.243},
	abstract = {In this work, we provide a systematic and comprehensive empirical comparison of pretrained multilingual language models versus their monolingual counterparts with regard to their monolingual task performance. We study a set of nine typologically diverse languages with readily available pretrained monolingual models on a set of five diverse monolingual downstream tasks. We first aim to establish, via fair and controlled comparisons, if a gap between the multilingual and the corresponding monolingual representation of that language exists, and subsequently investigate the reason for any performance difference. To disentangle conflating factors, we train new monolingual models on the same data, with monolingually and multilingually trained tokenizers. We find that while the pretraining data size is an important factor, a designated monolingual tokenizer plays an equally important role in the downstream performance. Our results show that languages that are adequately represented in the multilingual model's vocabulary exhibit negligible performance decreases over their monolingual counterparts. We further find that replacing the original multilingual tokenizer with the specialized monolingual tokenizer improves the downstream performance of the multilingual model for almost every task and language.},
	urldate = {2023-03-01},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Rust, Phillip and Pfeiffer, Jonas and Vulić, Ivan and Ruder, Sebastian and Gurevych, Iryna},
	month = aug,
	year = {2021},
	keywords = {tokenizers, analysis},
	pages = {3118--3135},
	file = {Full Text PDF:/home/jirka/Zotero/storage/PABC2IYG/Rust et al. - 2021 - How Good is Your Tokenizer On the Monolingual Per.pdf:application/pdf},
}

@misc{noauthor_tomlimientangled_in_scripts_nodate,
	title = {tomlimi/entangled\_in\_scripts},
	url = {https://github.com/tomlimi/entangled_in_scripts},
	abstract = {Contribute to tomlimi/entangled\_in\_scripts development by creating an account on GitHub.},
	language = {en},
	urldate = {2023-03-02},
	journal = {GitHub},
	file = {Snapshot:/home/jirka/Zotero/storage/7BP8QFP9/entangled_in_scripts.html:text/html},
}

@inproceedings{kumar_bpe_2022,
	address = {Dublin, Ireland},
	title = {{BPE} beyond {Word} {Boundary}: {How} {NOT} to use {Multi} {Word} {Expressions} in {Neural} {Machine} {Translation}},
	shorttitle = {{BPE} beyond {Word} {Boundary}},
	url = {https://aclanthology.org/2022.insights-1.24},
	doi = {10.18653/v1/2022.insights-1.24},
	abstract = {BPE tokenization merges characters into longer tokens by finding frequently occurring contiguous patterns within the word boundary. An intuitive relaxation would be to extend a BPE vocabulary with multi-word expressions (MWEs): bigrams (\$in\_a\$), trigrams (\$out\_of\_the\$), and skip-grams (\$he . his\$). In the context of Neural Machine Translation (NMT), we replace the least frequent subword/whole-word tokens with the most frequent MWEs. We find that these modifications to BPE end up hurting the model, resulting in a net drop of BLEU and chrF scores across two language pairs. We observe that naively extending BPE beyond word boundaries results in incoherent tokens which are themselves better represented as individual words. Moreover, we find that Pointwise Mutual Information (PMI) instead of frequency finds better MWEs (e.g., \$New\_York\$, \$Statue\_of\_Liberty\$, \$neither . nor\$) which consistently improves translation performance.We release all code at https://github.com/pegasus-lynx/mwe-bpe.},
	urldate = {2023-03-02},
	booktitle = {Proceedings of the {Third} {Workshop} on {Insights} from {Negative} {Results} in {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Kumar, Dipesh and Thawani, Avijit},
	month = may,
	year = {2022},
	keywords = {tokenizers, analysis},
	pages = {172--179},
	file = {Full Text PDF:/home/jirka/Zotero/storage/V9KNK6KP/Kumar and Thawani - 2022 - BPE beyond Word Boundary How NOT to use Multi Wor.pdf:application/pdf},
}

@inproceedings{hofmann_embarrassingly_2022,
	address = {Dublin, Ireland},
	title = {An {Embarrassingly} {Simple} {Method} to {Mitigate} {Undesirable} {Properties} of {Pretrained} {Language} {Model} {Tokenizers}},
	url = {https://aclanthology.org/2022.acl-short.43},
	doi = {10.18653/v1/2022.acl-short.43},
	abstract = {We introduce FLOTA (Few Longest Token Approximation), a simple yet effective method to improve the tokenization of pretrained language models (PLMs). FLOTA uses the vocabulary of a standard tokenizer but tries to preserve the morphological structure of words during tokenization. We evaluate FLOTA on morphological gold segmentations as well as a text classification task, using BERT, GPT-2, and XLNet as example PLMs. FLOTA leads to performance gains, makes inference more efficient, and enhances the robustness of PLMs with respect to whitespace noise.},
	urldate = {2023-03-04},
	booktitle = {Proceedings of the 60th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 2: {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Hofmann, Valentin and Schuetze, Hinrich and Pierrehumbert, Janet},
	month = may,
	year = {2022},
	keywords = {tokenizers, method},
	pages = {385--393},
	file = {Full Text PDF:/home/jirka/Zotero/storage/BCFDN8FE/Hofmann et al. - 2022 - An Embarrassingly Simple Method to Mitigate Undesi.pdf:application/pdf},
}

@inproceedings{kudo_sentencepiece_2018-1,
	address = {Brussels, Belgium},
	title = {{SentencePiece}: {A} simple and language independent subword tokenizer and detokenizer for {Neural} {Text} {Processing}},
	shorttitle = {{SentencePiece}},
	url = {https://aclanthology.org/D18-2012},
	doi = {10.18653/v1/D18-2012},
	abstract = {This paper describes SentencePiece, a language-independent subword tokenizer and detokenizer designed for Neural-based text processing, including Neural Machine Translation. It provides open-source C++ and Python implementations for subword units. While existing subword segmentation tools assume that the input is pre-tokenized into word sequences, SentencePiece can train subword models directly from raw sentences, which allows us to make a purely end-to-end and language independent system. We perform a validation experiment of NMT on English-Japanese machine translation, and find that it is possible to achieve comparable accuracy to direct subword training from raw sentences. We also compare the performance of subword training and segmentation with various configurations. SentencePiece is available under the Apache 2 license at https://github.com/google/sentencepiece.},
	urldate = {2023-03-05},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Kudo, Taku and Richardson, John},
	month = nov,
	year = {2018},
	keywords = {tokenizers, method},
	pages = {66--71},
	file = {Full Text PDF:/home/jirka/Zotero/storage/PB4TTAI4/Kudo and Richardson - 2018 - SentencePiece A simple and language independent s.pdf:application/pdf},
}

@inproceedings{banerjee_meaningless_2018,
	address = {New Orleans},
	title = {Meaningless yet meaningful: {Morphology} grounded subword-level {NMT}},
	shorttitle = {Meaningless yet meaningful},
	url = {https://aclanthology.org/W18-1207},
	doi = {10.18653/v1/W18-1207},
	abstract = {We explore the use of two independent subsystems Byte Pair Encoding (BPE) and Morfessor as basic units for subword-level neural machine translation (NMT). We show that, for linguistically distant language-pairs Morfessor-based segmentation algorithm produces significantly better quality translation than BPE. However, for close language-pairs BPE-based subword-NMT may translate better than Morfessor-based subword-NMT. We propose a combined approach of these two segmentation algorithms Morfessor-BPE (M-BPE) which outperforms these two baseline systems in terms of BLEU score. Our results are supported by experiments on three language-pairs: English-Hindi, Bengali-Hindi and English-Bengali.},
	urldate = {2023-03-08},
	booktitle = {Proceedings of the {Second} {Workshop} on {Subword}/{Character} {LEvel} {Models}},
	publisher = {Association for Computational Linguistics},
	author = {Banerjee, Tamali and Bhattacharyya, Pushpak},
	month = jun,
	year = {2018},
	keywords = {tokenizers},
	pages = {55--60},
	file = {Full Text PDF:/home/jirka/Zotero/storage/PFXIE8BV/Banerjee and Bhattacharyya - 2018 - Meaningless yet meaningful Morphology grounded su.pdf:application/pdf},
}

@inproceedings{klein_getting_2020,
	address = {Online},
	title = {Getting the \#\#life out of living: {How} {Adequate} {Are} {Word}-{Pieces} for {Modelling} {Complex} {Morphology}?},
	shorttitle = {Getting the \#\#life out of living},
	url = {https://aclanthology.org/2020.sigmorphon-1.24},
	doi = {10.18653/v1/2020.sigmorphon-1.24},
	abstract = {This work investigates the most basic units that underlie contextualized word embeddings, such as BERT — the so-called word pieces. In Morphologically-Rich Languages (MRLs) which exhibit morphological fusion and non-concatenative morphology, the different units of meaning within a word may be fused, intertwined, and cannot be separated linearly. Therefore, when using word-pieces in MRLs, we must consider that: (1) a linear segmentation into sub-word units might not capture the full morphological complexity of words; and (2) representations that leave morphological knowledge on sub-word units inaccessible might negatively affect performance. Here we empirically examine the capacity of word-pieces to capture morphology by investigating the task of multi-tagging in Modern Hebrew, as a proxy to evaluate the underlying segmentation. Our results show that, while models trained to predict multi-tags for complete words outperform models tuned to predict the distinct tags of WPs, we can improve the WPs tag prediction by purposefully constraining the word-pieces to reflect their internal functions. We suggest that linguistically-informed word-pieces schemes, that make the morphological structure explicit, might boost performance for MRLs.},
	urldate = {2023-03-08},
	booktitle = {Proceedings of the 17th {SIGMORPHON} {Workshop} on {Computational} {Research} in {Phonetics}, {Phonology}, and {Morphology}},
	publisher = {Association for Computational Linguistics},
	author = {Klein, Stav and Tsarfaty, Reut},
	month = jul,
	year = {2020},
	keywords = {tokenizers, analysis},
	pages = {204--209},
	file = {Full Text PDF:/home/jirka/Zotero/storage/4TKVN6TU/Klein and Tsarfaty - 2020 - Getting the ##life out of living How Adequate Are.pdf:application/pdf},
}

@inproceedings{hakimi_parizi_evaluating_2020,
	address = {Marseille, France},
	title = {Evaluating {Sub}-word {Embeddings} in {Cross}-lingual {Models}},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.330},
	abstract = {Cross-lingual word embeddings create a shared space for embeddings in two languages, and enable knowledge to be transferred between languages for tasks such as bilingual lexicon induction. One problem, however, is out-of-vocabulary (OOV) words, for which no embeddings are available. This is particularly problematic for low-resource and morphologically-rich languages, which often have relatively high OOV rates. Approaches to learning sub-word embeddings have been proposed to address the problem of OOV words, but most prior work has not considered sub-word embeddings in cross-lingual models. In this paper, we consider whether sub-word embeddings can be leveraged to form cross-lingual embeddings for OOV words. Specifically, we consider a novel bilingual lexicon induction task focused on OOV words, for language pairs covering several language families. Our results indicate that cross-lingual representations for OOV words can indeed be formed from sub-word embeddings, including in the case of a truly low-resource morphologically-rich language.},
	language = {English},
	urldate = {2023-03-08},
	booktitle = {Proceedings of the {Twelfth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Hakimi Parizi, Ali and Cook, Paul},
	month = may,
	year = {2020},
	pages = {2712--2719},
	file = {Full Text PDF:/home/jirka/Zotero/storage/HS2GS7VA/Hakimi Parizi and Cook - 2020 - Evaluating Sub-word Embeddings in Cross-lingual Mo.pdf:application/pdf},
}

@inproceedings{rust_how_2021-1,
	address = {Online},
	title = {How {Good} is {Your} {Tokenizer}? {On} the {Monolingual} {Performance} of {Multilingual} {Language} {Models}},
	shorttitle = {How {Good} is {Your} {Tokenizer}?},
	url = {https://aclanthology.org/2021.acl-long.243},
	doi = {10.18653/v1/2021.acl-long.243},
	abstract = {In this work, we provide a systematic and comprehensive empirical comparison of pretrained multilingual language models versus their monolingual counterparts with regard to their monolingual task performance. We study a set of nine typologically diverse languages with readily available pretrained monolingual models on a set of five diverse monolingual downstream tasks. We first aim to establish, via fair and controlled comparisons, if a gap between the multilingual and the corresponding monolingual representation of that language exists, and subsequently investigate the reason for any performance difference. To disentangle conflating factors, we train new monolingual models on the same data, with monolingually and multilingually trained tokenizers. We find that while the pretraining data size is an important factor, a designated monolingual tokenizer plays an equally important role in the downstream performance. Our results show that languages that are adequately represented in the multilingual model's vocabulary exhibit negligible performance decreases over their monolingual counterparts. We further find that replacing the original multilingual tokenizer with the specialized monolingual tokenizer improves the downstream performance of the multilingual model for almost every task and language.},
	urldate = {2023-03-08},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Rust, Phillip and Pfeiffer, Jonas and Vulić, Ivan and Ruder, Sebastian and Gurevych, Iryna},
	month = aug,
	year = {2021},
	pages = {3118--3135},
	file = {Full Text PDF:/home/jirka/Zotero/storage/JI6V6TUQ/Rust et al. - 2021 - How Good is Your Tokenizer On the Monolingual Per.pdf:application/pdf},
}

@inproceedings{maronikolakis_wine_2021,
	address = {Punta Cana, Dominican Republic},
	title = {Wine is not v i n. {On} the {Compatibility} of {Tokenizations} across {Languages}},
	url = {https://aclanthology.org/2021.findings-emnlp.205},
	doi = {10.18653/v1/2021.findings-emnlp.205},
	abstract = {The size of the vocabulary is a central design choice in large pretrained language models, with respect to both performance and memory requirements. Typically, subword tokenization algorithms such as byte pair encoding and WordPiece are used. In this work, we investigate the compatibility of tokenizations for multilingual static and contextualized embedding spaces and propose a measure that reflects the compatibility of tokenizations across languages. Our goal is to prevent incompatible tokenizations, e.g., “wine” (word-level) in English vs. “v i n” (character-level) in French, which make it hard to learn good multilingual semantic representations. We show that our compatibility measure allows the system designer to create vocabularies across languages that are compatible – a desideratum that so far has been neglected in multilingual models.},
	urldate = {2023-03-09},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Maronikolakis, Antonis and Dufter, Philipp and Schütze, Hinrich},
	month = nov,
	year = {2021},
	pages = {2382--2399},
	file = {Full Text PDF:/home/jirka/Zotero/storage/SXFTG6A7/Maronikolakis et al. - 2021 - Wine is not v i n. On the Compatibility of Tokeniz.pdf:application/pdf},
}

@inproceedings{maronikolakis_wine_2021-1,
	address = {Punta Cana, Dominican Republic},
	title = {Wine is not v i n. {On} the {Compatibility} of {Tokenizations} across {Languages}},
	url = {https://aclanthology.org/2021.findings-emnlp.205},
	doi = {10.18653/v1/2021.findings-emnlp.205},
	abstract = {The size of the vocabulary is a central design choice in large pretrained language models, with respect to both performance and memory requirements. Typically, subword tokenization algorithms such as byte pair encoding and WordPiece are used. In this work, we investigate the compatibility of tokenizations for multilingual static and contextualized embedding spaces and propose a measure that reﬂects the compatibility of tokenizations across languages. Our goal is to prevent incompatible tokenizations, e.g., "wine" (word-level) in English vs. "v i n" (character-level) in French, which make it hard to learn good multilingual semantic representations. We show that our compatibility measure allows the system designer to create vocabularies across languages that are compatible – a desideratum that so far has been neglected in multilingual models.},
	language = {en},
	urldate = {2023-03-09},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Maronikolakis, Antonis and Dufter, Philipp and Schütze, Hinrich},
	year = {2021},
	keywords = {tokenizers, analysis},
	pages = {2382--2399},
	file = {Maronikolakis et al. - 2021 - Wine is not v i n. On the Compatibility of Tokeniz.pdf:/home/jirka/Zotero/storage/XIWKVYNV/Maronikolakis et al. - 2021 - Wine is not v i n. On the Compatibility of Tokeniz.pdf:application/pdf},
}

@inproceedings{gowda_finding_2020,
	address = {Online},
	title = {Finding the {Optimal} {Vocabulary} {Size} for {Neural} {Machine} {Translation}},
	url = {https://aclanthology.org/2020.findings-emnlp.352},
	doi = {10.18653/v1/2020.findings-emnlp.352},
	abstract = {We cast neural machine translation (NMT) as a classification task in an autoregressive setting and analyze the limitations of both classification and autoregression components. Classifiers are known to perform better with balanced class distributions during training. Since the Zipfian nature of languages causes imbalanced classes, we explore its effect on NMT. We analyze the effect of various vocabulary sizes on NMT performance on multiple languages with many data sizes, and reveal an explanation for why certain vocabulary sizes are better than others.},
	urldate = {2023-03-23},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Gowda, Thamme and May, Jonathan},
	month = nov,
	year = {2020},
	keywords = {tokenizers, analysis},
	pages = {3955--3964},
	file = {Full Text PDF:/home/jirka/Zotero/storage/RQQGMIZS/Gowda and May - 2020 - Finding the Optimal Vocabulary Size for Neural Mac.pdf:application/pdf},
}

@inproceedings{k_cross-lingual_2022,
	title = {Cross-{Lingual} {Ability} of {Multilingual} {BERT}: {An} {Empirical} {Study}},
	shorttitle = {Cross-{Lingual} {Ability} of {Multilingual} {BERT}},
	url = {https://openreview.net/forum?id=HJeT3yrtDr},
	abstract = {Recent work has exhibited the surprising cross-lingual abilities of multilingual BERT (M-BERT) -- surprising since it is trained without any cross-lingual objective and with no aligned data. In this work, we provide a comprehensive study of the contribution of different components in M-BERT to its cross-lingual ability. We study the impact of linguistic properties of the languages, the architecture of the model, and the learning objectives. The experimental study is done in the context of three typologically different languages -- Spanish, Hindi, and Russian -- and using two conceptually different NLP tasks, textual entailment and named entity recognition. Among our key conclusions is the fact that the lexical overlap between languages plays a negligible role in the cross-lingual success, while the depth of the network is an integral part of it. All our models and implementations can be found on our project page: http://cogcomp.org/page/publication\_view/900.},
	language = {en},
	urldate = {2023-03-30},
	author = {K, Karthikeyan and Wang, Zihan and Mayhew, Stephen and Roth, Dan},
	month = feb,
	year = {2022},
	keywords = {analysis},
	file = {Full Text PDF:/home/jirka/Zotero/storage/85GKHT6K/K et al. - 2022 - Cross-Lingual Ability of Multilingual BERT An Emp.pdf:application/pdf},
}

@inproceedings{wang_improving_2019,
	address = {Hong Kong, China},
	title = {Improving {Pre}-{Trained} {Multilingual} {Model} with {Vocabulary} {Expansion}},
	url = {https://aclanthology.org/K19-1030},
	doi = {10.18653/v1/K19-1030},
	abstract = {Recently, pre-trained language models have achieved remarkable success in a broad range of natural language processing tasks. However, in multilingual setting, it is extremely resource-consuming to pre-train a deep language model over large-scale corpora for each language. Instead of exhaustively pre-training monolingual language models independently, an alternative solution is to pre-train a powerful multilingual deep language model over large-scale corpora in hundreds of languages. However, the vocabulary size for each language in such a model is relatively small, especially for low-resource languages. This limitation inevitably hinders the performance of these multilingual models on tasks such as sequence labeling, wherein in-depth token-level or sentence-level understanding is essential. In this paper, inspired by previous methods designed for monolingual settings, we investigate two approaches (i.e., joint mapping and mixture mapping) based on a pre-trained multilingual model BERT for addressing the out-of-vocabulary (OOV) problem on a variety of tasks, including part-of-speech tagging, named entity recognition, machine translation quality estimation, and machine reading comprehension. Experimental results show that using mixture mapping is more promising. To the best of our knowledge, this is the first work that attempts to address and discuss the OOV issue in multilingual settings.},
	urldate = {2023-03-30},
	booktitle = {Proceedings of the 23rd {Conference} on {Computational} {Natural} {Language} {Learning} ({CoNLL})},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Hai and Yu, Dian and Sun, Kai and Chen, Jianshu and Yu, Dong},
	month = nov,
	year = {2019},
	keywords = {tokenizers, method},
	pages = {316--327},
	file = {Full Text PDF:/home/jirka/Zotero/storage/C5IWTLTA/Wang et al. - 2019 - Improving Pre-Trained Multilingual Model with Voca.pdf:application/pdf},
}

@inproceedings{chen_how_2019,
	address = {Minneapolis, Minnesota},
	title = {How {Large} a {Vocabulary} {Does} {Text} {Classification} {Need}? {A} {Variational} {Approach} to {Vocabulary} {Selection}},
	shorttitle = {How {Large} a {Vocabulary} {Does} {Text} {Classification} {Need}?},
	url = {http://aclweb.org/anthology/N19-1352},
	doi = {10.18653/v1/N19-1352},
	abstract = {With the rapid development in deep learning, deep neural networks have been widely adopted in many real-life natural language applications. Under deep neural networks, a predeﬁned vocabulary is required to vectorize text inputs. The canonical approach to select predeﬁned vocabulary is based on the word frequency, where a threshold is selected to cut off the long tail distribution. However, we observed that such a simple approach could easily lead to under-sized vocabulary or oversized vocabulary issues. Therefore, we are interested in understanding how the end-task classiﬁcation accuracy is related to the vocabulary size and what is the minimum required vocabulary size to achieve a speciﬁc performance. In this paper, we provide a more sophisticated variational vocabulary dropout (VVD) based on variational dropout to perform vocabulary selection, which can intelligently select the subset of the vocabulary to achieve the required performance. To evaluate different algorithms on the newly proposed vocabulary selection problem, we propose two new metrics: Area Under AccuracyVocab Curve and Vocab Size under X\% Accuracy Drop. Through extensive experiments on various NLP classiﬁcation tasks, our variational framework is shown to signiﬁcantly outperform the frequency-based and other selection baselines on these metrics.},
	language = {en},
	urldate = {2023-04-09},
	booktitle = {Proceedings of the 2019 {Conference} of the {North}},
	publisher = {Association for Computational Linguistics},
	author = {Chen, Wenhu and Su, Yu and Shen, Yilin and Chen, Zhiyu and Yan, Xifeng and Wang, William Yang},
	year = {2019},
	pages = {3487--3497},
	file = {Chen et al. - 2019 - How Large a Vocabulary Does Text Classification Ne.pdf:/home/jirka/Zotero/storage/SDYY2E4N/Chen et al. - 2019 - How Large a Vocabulary Does Text Classification Ne.pdf:application/pdf},
}

@inproceedings{sachidananda_efficient_2021,
	address = {Virtual},
	title = {Efficient {Domain} {Adaptation} of {Language} {Models} via {Adaptive} {Tokenization}},
	url = {https://aclanthology.org/2021.sustainlp-1.16},
	doi = {10.18653/v1/2021.sustainlp-1.16},
	language = {en},
	urldate = {2023-04-09},
	booktitle = {Proceedings of the {Second} {Workshop} on {Simple} and {Efficient} {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Sachidananda, Vin and Kessler, Jason and Lai, Yi-An},
	year = {2021},
	keywords = {tokenizers, domains, method},
	pages = {155--165},
	file = {Sachidananda et al. - 2021 - Efficient Domain Adaptation of Language Models via.pdf:/home/jirka/Zotero/storage/579M462C/Sachidananda et al. - 2021 - Efficient Domain Adaptation of Language Models via.pdf:application/pdf},
}

@inproceedings{gururangan_dont_2020,
	address = {Online},
	title = {Don't {Stop} {Pretraining}: {Adapt} {Language} {Models} to {Domains} and {Tasks}},
	shorttitle = {Don't {Stop} {Pretraining}},
	url = {https://aclanthology.org/2020.acl-main.740},
	doi = {10.18653/v1/2020.acl-main.740},
	abstract = {Language models pretrained on text from a wide variety of sources form the foundation of today's NLP. In light of the success of these broad-coverage models, we investigate whether it is still helpful to tailor a pretrained model to the domain of a target task. We present a study across four domains (biomedical and computer science publications, news, and reviews) and eight classification tasks, showing that a second phase of pretraining in-domain (domain-adaptive pretraining) leads to performance gains, under both high- and low-resource settings. Moreover, adapting to the task's unlabeled data (task-adaptive pretraining) improves performance even after domain-adaptive pretraining. Finally, we show that adapting to a task corpus augmented using simple data selection strategies is an effective alternative, especially when resources for domain-adaptive pretraining might be unavailable. Overall, we consistently find that multi-phase adaptive pretraining offers large gains in task performance.},
	urldate = {2023-04-13},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Gururangan, Suchin and Marasović, Ana and Swayamdipta, Swabha and Lo, Kyle and Beltagy, Iz and Downey, Doug and Smith, Noah A.},
	month = jul,
	year = {2020},
	keywords = {domains, analysis},
	pages = {8342--8360},
	file = {Byte_Pair_Encoding_is_Approximately_Optimal.pdf:/home/jirka/Zotero/storage/KM9AQP5A/Byte_Pair_Encoding_is_Approximately_Optimal.pdf:application/pdf;Full Text PDF:/home/jirka/Zotero/storage/8JLBXR9V/Gururangan et al. - 2020 - Don't Stop Pretraining Adapt Language Models to D.pdf:application/pdf},
}

@misc{ma_searching_2022,
	title = {Searching for {Optimal} {Subword} {Tokenization} in {Cross}-domain {NER}},
	url = {http://arxiv.org/abs/2206.03352},
	abstract = {Input distribution shift is one of the vital problems in unsupervised domain adaptation (UDA). The most popular UDA approaches focus on domain-invariant representation learning, trying to align the features from different domains into similar feature distributions. However, these approaches ignore the direct alignment of input word distributions between domains, which is a vital factor in word-level classiﬁcation tasks such as cross-domain NER. In this work, we shed new light on cross-domain NER by introducing a subword-level solution, X-Piece, for input word-level distribution shift in NER. Speciﬁcally, we re-tokenize the input words of the source domain to approach the target subword distribution, which is formulated and solved as an optimal transport problem. As this approach focuses on the input level, it can also be combined with previous DIRL methods for further improvement. Experimental results show the effectiveness of the proposed method based on BERT-tagger on four benchmark NER datasets. Also, the proposed method is proved to beneﬁt DIRL methods such as DANN.},
	language = {en},
	urldate = {2023-04-13},
	publisher = {arXiv},
	author = {Ma, Ruotian and Tan, Yiding and Zhou, Xin and Chen, Xuanting and Liang, Di and Wang, Sirui and Wu, Wei and Gui, Tao and Zhang, Qi},
	month = jun,
	year = {2022},
	note = {arXiv:2206.03352 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, tokenizers},
	file = {Ma et al. - 2022 - Searching for Optimal Subword Tokenization in Cros.pdf:/home/jirka/Zotero/storage/7DW4XHXN/Ma et al. - 2022 - Searching for Optimal Subword Tokenization in Cros.pdf:application/pdf},
}

@misc{gururangan_whose_2022,
	title = {Whose {Language} {Counts} as {High} {Quality}? {Measuring} {Language} {Ideologies} in {Text} {Data} {Selection}},
	shorttitle = {Whose {Language} {Counts} as {High} {Quality}?},
	url = {http://arxiv.org/abs/2201.10474},
	abstract = {Language models increasingly rely on massive web dumps for diverse text data. However, these sources are rife with undesirable content. As such, resources like Wikipedia, books, and newswire often serve as anchors for automatically selecting web text most suitable for language modeling, a process typically referred to as quality ﬁltering. Using a new dataset of U.S. high school newspaper articles—written by students from across the country—we investigate whose language is preferred by the quality ﬁlter used for GPT-3. We ﬁnd that newspapers from larger schools, located in wealthier, educated, and urban ZIP codes are more likely to be classiﬁed as high quality. We then demonstrate that the ﬁlter’s measurement of quality is unaligned with other sensible metrics, such as factuality or literary acclaim. We argue that privileging any corpus as high quality entails a language ideology, and more care is needed to construct training corpora for language models, with better transparency and justiﬁcation for the inclusion or exclusion of various texts.},
	language = {en},
	urldate = {2023-04-14},
	publisher = {arXiv},
	author = {Gururangan, Suchin and Card, Dallas and Dreier, Sarah K. and Gade, Emily K. and Wang, Leroy Z. and Wang, Zeyu and Zettlemoyer, Luke and Smith, Noah A.},
	month = jan,
	year = {2022},
	note = {arXiv:2201.10474 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, analysis},
	file = {Gururangan et al. - 2022 - Whose Language Counts as High Quality Measuring L.pdf:/home/jirka/Zotero/storage/UGQLK2DN/Gururangan et al. - 2022 - Whose Language Counts as High Quality Measuring L.pdf:application/pdf},
}

@misc{gururangan_scaling_2023,
	title = {Scaling {Expert} {Language} {Models} with {Unsupervised} {Domain} {Discovery}},
	url = {http://arxiv.org/abs/2303.14177},
	abstract = {Large language models are typically trained densely: all parameters are updated with respect to all inputs. This requires synchronization of billions of parameters across thousands of GPUs. We introduce a simple but effective method to asynchronously train large, sparse language models on arbitrary text corpora. Our method clusters a corpus into sets of related documents, trains a separate expert language model on each cluster, and combines them in a sparse ensemble for inference. This approach generalizes embarrassingly parallel training by automatically discovering the domains for each expert, and eliminates nearly all the communication overhead of existing sparse language models. Our technique outperforms dense baselines on multiple corpora and few-shot tasks, and our analysis shows that specializing experts to meaningful clusters is key to these gains. Performance also improves with the number of experts and size of training data, suggesting this is a highly efficient and accessible approach to training large language models.},
	language = {en},
	urldate = {2023-04-14},
	publisher = {arXiv},
	author = {Gururangan, Suchin and Li, Margaret and Lewis, Mike and Shi, Weijia and Althoff, Tim and Smith, Noah A. and Zettlemoyer, Luke},
	month = mar,
	year = {2023},
	note = {arXiv:2303.14177 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Artificial Intelligence, domains},
	file = {Gururangan et al. - 2023 - Scaling Expert Language Models with Unsupervised D.pdf:/home/jirka/Zotero/storage/BMGHT8S7/Gururangan et al. - 2023 - Scaling Expert Language Models with Unsupervised D.pdf:application/pdf},
}

@misc{xie_data_2023,
	title = {Data {Selection} for {Language} {Models} via {Importance} {Resampling}},
	url = {http://arxiv.org/abs/2302.03169},
	abstract = {Selecting a suitable training dataset is crucial for both general-domain (e.g., GPT-3) and domain-speciﬁc (e.g., Codex) language models (LMs). We formalize this data selection problem as selecting a subset of a large raw unlabeled dataset to match a desired target distribution, given some unlabeled target samples. Due to the large scale and dimensionality of the raw text data, existing methods use simple heuristics to select data that are similar to a high-quality reference corpus (e.g., Wikipedia), or leverage experts to manually curate data. Instead, we extend the classic importance resampling approach used in low-dimensions for LM data selection. Crucially, we work in a reduced feature space to make importance weight estimation tractable over the space of text. To determine an appropriate feature space, we ﬁrst show that KL reduction, a data metric that measures the proximity between selected data and the target in a feature space, has high correlation with average accuracy on 8 downstream tasks (r = 0.89) when computed with simple n-gram features. From this observation, we present Data Selection with Importance Resampling (DSIR), an efﬁcient and scalable algorithm that estimates importance weights in a reduced feature space (e.g., n-gram features in our instantiation) and selects data with importance resampling according to these weights. When training general-domain models (target is Wikipedia + books), DSIR improves over random selection and heuristic ﬁltering baselines by 2–2.5\% on the GLUE benchmark. When performing continued pretraining towards a speciﬁc domain, DSIR performs comparably to expert curated data across 8 target distributions.},
	language = {en},
	urldate = {2023-04-14},
	publisher = {arXiv},
	author = {Xie, Sang Michael and Santurkar, Shibani and Ma, Tengyu and Liang, Percy},
	month = feb,
	year = {2023},
	note = {arXiv:2302.03169 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, domains},
	file = {Xie et al. - 2023 - Data Selection for Language Models via Importance .pdf:/home/jirka/Zotero/storage/4I279EF3/Xie et al. - 2023 - Data Selection for Language Models via Importance .pdf:application/pdf},
}

@inproceedings{sennrich_neural_2016,
	address = {Berlin, Germany},
	title = {Neural {Machine} {Translation} of {Rare} {Words} with {Subword} {Units}},
	url = {https://aclanthology.org/P16-1162},
	doi = {10.18653/v1/P16-1162},
	urldate = {2023-04-24},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
	month = aug,
	year = {2016},
	keywords = {background, tokenizers, method},
	pages = {1715--1725},
	file = {Full Text PDF:/home/jirka/Zotero/storage/4Y7QVKWL/Sennrich et al. - 2016 - Neural Machine Translation of Rare Words with Subw.pdf:application/pdf},
}

@inproceedings{devlin_bert_2019,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {https://aclanthology.org/N19-1423},
	doi = {10.18653/v1/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2023-05-14},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = jun,
	year = {2019},
	pages = {4171--4186},
	file = {Full Text PDF:/home/jirka/Zotero/storage/ZFSEDSKM/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf},
}

@inproceedings{devlin_bert_2019-1,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of {Deep} {Bidirectional} {Transformers} for {Language} {Understanding}},
	shorttitle = {{BERT}},
	url = {https://aclanthology.org/N19-1423},
	doi = {10.18653/v1/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2023-05-14},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = jun,
	year = {2019},
	keywords = {model},
	pages = {4171--4186},
	file = {Full Text PDF:/home/jirka/Zotero/storage/AJD8LDZD/Devlin et al. - 2019 - BERT Pre-training of Deep Bidirectional Transform.pdf:application/pdf},
}

@article{liu_multilingual_2020,
	title = {Multilingual {Denoising} {Pre}-training for {Neural} {Machine} {Translation}},
	volume = {8},
	url = {https://aclanthology.org/2020.tacl-1.47},
	doi = {10.1162/tacl_a_00343},
	abstract = {This paper demonstrates that multilingual denoising pre-training produces significant performance gains across a wide variety of machine translation (MT) tasks. We present mBART—a sequence-to-sequence denoising auto-encoder pre-trained on large-scale monolingual corpora in many languages using the BART objective (Lewis et al., 2019). mBART is the first method for pre-training a complete sequence-to-sequence model by denoising full texts in multiple languages, whereas previous approaches have focused only on the encoder, decoder, or reconstructing parts of the text. Pre-training a complete model allows it to be directly fine-tuned for supervised (both sentence-level and document-level) and unsupervised machine translation, with no task- specific modifications. We demonstrate that adding mBART initialization produces performance gains in all but the highest-resource settings, including up to 12 BLEU points for low resource MT and over 5 BLEU points for many document-level and unsupervised models. We also show that it enables transfer to language pairs with no bi-text or that were not in the pre-training corpus, and present extensive analysis of which factors contribute the most to effective pre-training.1},
	urldate = {2023-05-14},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Liu, Yinhan and Gu, Jiatao and Goyal, Naman and Li, Xian and Edunov, Sergey and Ghazvininejad, Marjan and Lewis, Mike and Zettlemoyer, Luke},
	year = {2020},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	keywords = {model},
	pages = {726--742},
	file = {Full Text PDF:/home/jirka/Zotero/storage/2WLKQGGN/Liu et al. - 2020 - Multilingual Denoising Pre-training for Neural Mac.pdf:application/pdf},
}

@inproceedings{xue_mt5_2021,
	address = {Online},
	title = {{mT5}: {A} {Massively} {Multilingual} {Pre}-trained {Text}-to-{Text} {Transformer}},
	shorttitle = {{mT5}},
	url = {https://aclanthology.org/2021.naacl-main.41},
	doi = {10.18653/v1/2021.naacl-main.41},
	abstract = {The recent “Text-to-Text Transfer Transformer” (T5) leveraged a unified text-to-text format and scale to attain state-of-the-art results on a wide variety of English-language NLP tasks. In this paper, we introduce mT5, a multilingual variant of T5 that was pre-trained on a new Common Crawl-based dataset covering 101 languages. We detail the design and modified training of mT5 and demonstrate its state-of-the-art performance on many multilingual benchmarks. We also describe a simple technique to prevent “accidental translation” in the zero-shot setting, where a generative model chooses to (partially) translate its prediction into the wrong language. All of the code and model checkpoints used in this work are publicly available.},
	urldate = {2023-05-14},
	booktitle = {Proceedings of the 2021 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Xue, Linting and Constant, Noah and Roberts, Adam and Kale, Mihir and Al-Rfou, Rami and Siddhant, Aditya and Barua, Aditya and Raffel, Colin},
	month = jun,
	year = {2021},
	keywords = {model},
	pages = {483--498},
	file = {Full Text PDF:/home/jirka/Zotero/storage/ATPV86T7/Xue et al. - 2021 - mT5 A Massively Multilingual Pre-trained Text-to-.pdf:application/pdf},
}

@misc{acs_exploring_2019,
	title = {Exploring {BERT}'s {Vocabulary}},
	url = {https://juditacs.github.io/2019/02/19/bert-tokenization-stats.html},
	urldate = {2023-05-14},
	author = {Ács, Judit},
	month = feb,
	year = {2019},
	keywords = {tokenizers, analysis},
	file = {Exploring BERT's Vocabulary:/home/jirka/Zotero/storage/2MWAWTMX/bert-tokenization-stats.html:text/html},
}

@misc{fan_bloom_2023,
	title = {{BLOOM}: {A} {176B}-{Parameter} {Open}-{Access} {Multilingual} {Language} {Model}},
	shorttitle = {{BLOOM}},
	url = {http://arxiv.org/abs/2211.05100},
	abstract = {Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.},
	language = {en},
	urldate = {2023-05-15},
	publisher = {arXiv},
	author = {Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ilić, Suzana and Hesslow, Daniel and Castagné, Roman and Luccioni, Alexandra Sasha and Yvon, François and Gallé, Matthias and Tow, Jonathan and Rush, Alexander M. and Biderman, Stella and Webson, Albert and Ammanamanchi, Pawan Sasanka and Wang, Thomas and Sagot, Benoît and Muennighoff, Niklas and del Moral, Albert Villanova and Ruwase, Olatunji and Bawden, Rachel and Bekman, Stas and McMillan-Major, Angelina and Beltagy, Iz and Nguyen, Huu and Saulnier, Lucile and Tan, Samson and Suarez, Pedro Ortiz and Sanh, Victor and Laurençon, Hugo and Jernite, Yacine and Launay, Julien and Mitchell, Margaret and Raffel, Colin and Gokaslan, Aaron and Simhi, Adi and Soroa, Aitor and Aji, Alham Fikri and Alfassy, Amit and Rogers, Anna and Nitzav, Ariel Kreisberg and Xu, Canwen and Mou, Chenghao and Emezue, Chris and Klamm, Christopher and Leong, Colin and van Strien, Daniel and Adelani, David Ifeoluwa and Radev, Dragomir and Ponferrada, Eduardo González and Levkovizh, Efrat and Kim, Ethan and Natan, Eyal Bar and De Toni, Francesco and Dupont, Gérard and Kruszewski, Germán and Pistilli, Giada and Elsahar, Hady and Benyamina, Hamza and Tran, Hieu and Yu, Ian and Abdulmumin, Idris and Johnson, Isaac and Gonzalez-Dios, Itziar and de la Rosa, Javier and Chim, Jenny and Dodge, Jesse and Zhu, Jian and Chang, Jonathan and Frohberg, Jörg and Tobing, Joseph and Bhattacharjee, Joydeep and Almubarak, Khalid and Chen, Kimbo and Lo, Kyle and Von Werra, Leandro and Weber, Leon and Phan, Long and allal, Loubna Ben and Tanguy, Ludovic and Dey, Manan and Muñoz, Manuel Romero and Masoud, Maraim and Grandury, María and Šaško, Mario and Huang, Max and Coavoux, Maximin and Singh, Mayank and Jiang, Mike Tian-Jian and Vu, Minh Chien and Jauhar, Mohammad A. and Ghaleb, Mustafa and Subramani, Nishant and Kassner, Nora and Khamis, Nurulaqilla and Nguyen, Olivier and Espejel, Omar and de Gibert, Ona and Villegas, Paulo and Henderson, Peter and Colombo, Pierre and Amuok, Priscilla and Lhoest, Quentin and Harliman, Rheza and Bommasani, Rishi and López, Roberto Luis and Ribeiro, Rui and Osei, Salomey and Pyysalo, Sampo and Nagel, Sebastian and Bose, Shamik and Muhammad, Shamsuddeen Hassan and Sharma, Shanya and Longpre, Shayne and Nikpoor, Somaieh and Silberberg, Stanislav and Pai, Suhas and Zink, Sydney and Torrent, Tiago Timponi and Schick, Timo and Thrush, Tristan and Danchev, Valentin and Nikoulina, Vassilina and Laippala, Veronika and Lepercq, Violette and Prabhu, Vrinda and Alyafeai, Zaid and Talat, Zeerak and Raja, Arun and Heinzerling, Benjamin and Si, Chenglei and Taşar, Davut Emre and Salesky, Elizabeth and Mielke, Sabrina J. and Lee, Wilson Y. and Sharma, Abheesht and Santilli, Andrea and Chaffin, Antoine and Stiegler, Arnaud and Datta, Debajyoti and Szczechla, Eliza and Chhablani, Gunjan and Wang, Han and Pandey, Harshit and Strobelt, Hendrik and Fries, Jason Alan and Rozen, Jos and Gao, Leo and Sutawika, Lintang and Bari, M. Saiful and Al-shaibani, Maged S. and Manica, Matteo and Nayak, Nihal and Teehan, Ryan and Albanie, Samuel and Shen, Sheng and Ben-David, Srulik and Bach, Stephen H. and Kim, Taewoon and Bers, Tali and Fevry, Thibault and Neeraj, Trishala and Thakker, Urmish and Raunak, Vikas and Tang, Xiangru and Yong, Zheng-Xin and Sun, Zhiqing and Brody, Shaked and Uri, Yallow and Tojarieh, Hadar and Roberts, Adam and Chung, Hyung Won and Tae, Jaesung and Phang, Jason and Press, Ofir and Li, Conglong and Narayanan, Deepak and Bourfoune, Hatim and Casper, Jared and Rasley, Jeff and Ryabinin, Max and Mishra, Mayank and Zhang, Minjia and Shoeybi, Mohammad and Peyrounette, Myriam and Patry, Nicolas and Tazi, Nouamane and Sanseviero, Omar and von Platen, Patrick and Cornette, Pierre and Lavallée, Pierre François and Lacroix, Rémi and Rajbhandari, Samyam and Gandhi, Sanchit and Smith, Shaden and Requena, Stéphane and Patil, Suraj and Dettmers, Tim and Baruwa, Ahmed and Singh, Amanpreet and Cheveleva, Anastasia and Ligozat, Anne-Laure and Subramonian, Arjun and Névéol, Aurélie and Lovering, Charles and Garrette, Dan and Tunuguntla, Deepak and Reiter, Ehud and Taktasheva, Ekaterina and Voloshina, Ekaterina and Bogdanov, Eli and Winata, Genta Indra and Schoelkopf, Hailey and Kalo, Jan-Christoph and Novikova, Jekaterina and Forde, Jessica Zosa and Clive, Jordan and Kasai, Jungo and Kawamura, Ken and Hazan, Liam and Carpuat, Marine and Clinciu, Miruna and Kim, Najoung and Cheng, Newton and Serikov, Oleg and Antverg, Omer and van der Wal, Oskar and Zhang, Rui and Zhang, Ruochen and Gehrmann, Sebastian and Mirkin, Shachar and Pais, Shani and Shavrina, Tatiana and Scialom, Thomas and Yun, Tian and Limisiewicz, Tomasz and Rieser, Verena and Protasov, Vitaly and Mikhailov, Vladislav and Pruksachatkun, Yada and Belinkov, Yonatan and Bamberger, Zachary and Kasner, Zdeněk and Rueda, Alice and Pestana, Amanda and Feizpour, Amir and Khan, Ammar and Faranak, Amy and Santos, Ana and Hevia, Anthony and Unldreaj, Antigona and Aghagol, Arash and Abdollahi, Arezoo and Tammour, Aycha and HajiHosseini, Azadeh and Behroozi, Bahareh and Ajibade, Benjamin and Saxena, Bharat and Ferrandis, Carlos Muñoz and Contractor, Danish and Lansky, David and David, Davis and Kiela, Douwe and Nguyen, Duong A. and Tan, Edward and Baylor, Emi and Ozoani, Ezinwanne and Mirza, Fatima and Ononiwu, Frankline and Rezanejad, Habib and Jones, Hessie and Bhattacharya, Indrani and Solaiman, Irene and Sedenko, Irina and Nejadgholi, Isar and Passmore, Jesse and Seltzer, Josh and Sanz, Julio Bonis and Dutra, Livia and Samagaio, Mairon and Elbadri, Maraim and Mieskes, Margot and Gerchick, Marissa and Akinlolu, Martha and McKenna, Michael and Qiu, Mike and Ghauri, Muhammed and Burynok, Mykola and Abrar, Nafis and Rajani, Nazneen and Elkott, Nour and Fahmy, Nour and Samuel, Olanrewaju and An, Ran and Kromann, Rasmus and Hao, Ryan and Alizadeh, Samira and Shubber, Sarmad and Wang, Silas and Roy, Sourav and Viguier, Sylvain and Le, Thanh and Oyebade, Tobi and Le, Trieu and Yang, Yoyo and Nguyen, Zach and Kashyap, Abhinav Ramesh and Palasciano, Alfredo and Callahan, Alison and Shukla, Anima and Miranda-Escalada, Antonio and Singh, Ayush and Beilharz, Benjamin and Wang, Bo and Brito, Caio and Zhou, Chenxi and Jain, Chirag and Xu, Chuxin and Fourrier, Clémentine and Periñán, Daniel León and Molano, Daniel and Yu, Dian and Manjavacas, Enrique and Barth, Fabio and Fuhrimann, Florian and Altay, Gabriel and Bayrak, Giyaseddin and Burns, Gully and Vrabec, Helena U. and Bello, Imane and Dash, Ishani and Kang, Jihyun and Giorgi, John and Golde, Jonas and Posada, Jose David and Sivaraman, Karthik Rangasai and Bulchandani, Lokesh and Liu, Lu and Shinzato, Luisa and de Bykhovetz, Madeleine Hahn and Takeuchi, Maiko and Pàmies, Marc and Castillo, Maria A. and Nezhurina, Marianna and Sänger, Mario and Samwald, Matthias and Cullan, Michael and Weinberg, Michael and De Wolf, Michiel and Mihaljcic, Mina and Liu, Minna and Freidank, Moritz and Kang, Myungsun and Seelam, Natasha and Dahlberg, Nathan and Broad, Nicholas Michio and Muellner, Nikolaus and Fung, Pascale and Haller, Patrick and Chandrasekhar, Ramya and Eisenberg, Renata and Martin, Robert and Canalli, Rodrigo and Su, Rosaline and Su, Ruisi and Cahyawijaya, Samuel and Garda, Samuele and Deshmukh, Shlok S. and Mishra, Shubhanshu and Kiblawi, Sid and Ott, Simon and Sang-aroonsiri, Sinee and Kumar, Srishti and Schweter, Stefan and Bharati, Sushil and Laud, Tanmay and Gigant, Théo and Kainuma, Tomoya and Kusa, Wojciech and Labrak, Yanis and Bajaj, Yash Shailesh and Venkatraman, Yash and Xu, Yifan and Xu, Yingxin and Xu, Yu and Tan, Zhe and Xie, Zhongli and Ye, Zifan and Bras, Mathilde and Belkada, Younes and Wolf, Thomas},
	month = mar,
	year = {2023},
	note = {arXiv:2211.05100 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {Workshop et al. - 2023 - BLOOM A 176B-Parameter Open-Access Multilingual L.pdf:/home/jirka/Zotero/storage/5TAGNKUJ/Workshop et al. - 2023 - BLOOM A 176B-Parameter Open-Access Multilingual L.pdf:application/pdf},
}

@article{scao_bloom_2022,
	title = {Bloom: {A} 176b-parameter open-access multilingual language model},
	journal = {arXiv preprint arXiv:2211.05100},
	author = {Scao, Teven Le and Fan, Angela and Akiki, Christopher and Pavlick, Ellie and Ilić, Suzana and Hesslow, Daniel and Castagné, Roman and Luccioni, Alexandra Sasha and Yvon, François and Gallé, Matthias and {others}},
	year = {2022},
	keywords = {model},
}

@inproceedings{ruder_xtreme-r_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {{XTREME}-{R}: {Towards} {More} {Challenging} and {Nuanced} {Multilingual} {Evaluation}},
	shorttitle = {{XTREME}-{R}},
	url = {https://aclanthology.org/2021.emnlp-main.802},
	doi = {10.18653/v1/2021.emnlp-main.802},
	language = {en},
	urldate = {2023-05-18},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Ruder, Sebastian and Constant, Noah and Botha, Jan and Siddhant, Aditya and Firat, Orhan and Fu, Jinlan and Liu, Pengfei and Hu, Junjie and Garrette, Dan and Neubig, Graham and Johnson, Melvin},
	year = {2021},
	keywords = {evaluation},
	pages = {10215--10245},
	file = {Ruder et al. - 2021 - XTREME-R Towards More Challenging and Nuanced Mul.pdf:/home/jirka/Zotero/storage/56DP6KZI/Ruder et al. - 2021 - XTREME-R Towards More Challenging and Nuanced Mul.pdf:application/pdf},
}

@inproceedings{peters_deep_2018,
	address = {New Orleans, Louisiana},
	title = {Deep {Contextualized} {Word} {Representations}},
	url = {https://aclanthology.org/N18-1202},
	doi = {10.18653/v1/N18-1202},
	abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
	urldate = {2023-06-16},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	month = jun,
	year = {2018},
	keywords = {model},
	pages = {2227--2237},
	file = {Full Text PDF:/home/jirka/Zotero/storage/YTUC8FNE/Peters et al. - 2018 - Deep Contextualized Word Representations.pdf:application/pdf},
}

@inproceedings{zhang_how_2022,
	address = {Orlando, USA},
	title = {How {Robust} is {Neural} {Machine} {Translation} to {Language} {Imbalance} in {Multilingual} {Tokenizer} {Training}?},
	url = {https://aclanthology.org/2022.amta-research.8},
	abstract = {A multilingual tokenizer is a fundamental component of multilingual neural machine translation. It is trained from a multilingual corpus. Since a skewed data distribution is considered to be harmful, a sampling strategy is usually used to balance languages in the corpus. However, few works have systematically answered how language imbalance in tokenizer training affects downstream performance. In this work, we analyze how translation performance changes as the data ratios among languages vary in the tokenizer training corpus. We find that while relatively better performance is often observed when languages are more equally sampled, the downstream performance is more robust to language imbalance than we usually expected. Two features, UNK rate and closeness to the character level, can warn of poor downstream performance before performing the task. We also distinguish language sampling for tokenizer training from sampling for model training and show that the model is more sensitive to the latter.},
	urldate = {2023-06-17},
	booktitle = {Proceedings of the 15th biennial conference of the {Association} for {Machine} {Translation} in the {Americas} ({Volume} 1: {Research} {Track})},
	publisher = {Association for Machine Translation in the Americas},
	author = {Zhang, Shiyue and Chaudhary, Vishrav and Goyal, Naman and Cross, James and Wenzek, Guillaume and Bansal, Mohit and Guzman, Francisco},
	month = sep,
	year = {2022},
	keywords = {tokenizers, analysis},
	pages = {97--116},
	file = {Full Text PDF:/home/jirka/Zotero/storage/URB788SU/Zhang et al. - 2022 - How Robust is Neural Machine Translation to Langua.pdf:application/pdf},
}

@misc{jacobs_lost_2022,
	title = {Lost in {Space} {Marking}},
	url = {http://arxiv.org/abs/2208.01561},
	abstract = {We look at a decision taken early in training a subword tokenizer, namely whether it should be the word-initial token that carries a special mark, or the word-ﬁnal one. Based on surfacelevel considerations of efﬁciency and cohesion, as well as morphological coverage, we ﬁnd that a Unigram LM tokenizer trained on pretokenized English text is better off marking the word-initial token, while one trained on raw text beneﬁts from marking word ends. Our ﬁndings generalize across domains.},
	language = {en},
	urldate = {2023-06-17},
	publisher = {arXiv},
	author = {Jacobs, Cassandra L. and Pinter, Yuval},
	month = aug,
	year = {2022},
	note = {arXiv:2208.01561 [cs]},
	keywords = {Computer Science - Computation and Language, tokenizers, analysis},
	file = {Jacobs and Pinter - 2022 - Lost in Space Marking.pdf:/home/jirka/Zotero/storage/IUVDVCN5/Jacobs and Pinter - 2022 - Lost in Space Marking.pdf:application/pdf},
}

@book{noauthor_notitle_nodate,
}

@book{manning_foundations_1999,
	address = {Cambridge, Massachusetts},
	title = {Foundations of {Statistical} {Natural} {Language} {Processing}},
	url = {http://nlp.stanford.edu/fsnlp/},
	publisher = {The MIT Press},
	author = {Manning, Christopher D. and Schütze, Hinrich},
	year = {1999},
	keywords = {lecture nlp},
	file = {Christopher_D._Manning_Hinrich_Schütze_Foundations_Of_Statistical_Natural_Language_Processing.pdf:/home/jirka/Zotero/storage/MECY7I8N/Christopher_D._Manning_Hinrich_Schütze_Foundations_Of_Statistical_Natural_Language_Processing.pdf:application/pdf},
}

@article{radford_improving_2018,
	title = {Improving language understanding by generative pre-training},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
	year = {2018},
	keywords = {model, final thema:transformer},
	file = {Radford et al. - Improving Language Understanding by Generative Pre.pdf:/home/jirka/Zotero/storage/KRZGNMCZ/Radford et al. - Improving Language Understanding by Generative Pre.pdf:application/pdf},
}

@article{pedregosa_scikit-learn_2011,
	title = {Scikit-learn: {Machine} {Learning} in {Python}},
	volume = {12},
	journal = {Journal of Machine Learning Research},
	author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	year = {2011},
	pages = {2825--2830},
}

@inproceedings{wu_beto_2019,
	address = {Hong Kong, China},
	title = {Beto, {Bentz}, {Becas}: {The} {Surprising} {Cross}-{Lingual} {Effectiveness} of {BERT}},
	shorttitle = {Beto, {Bentz}, {Becas}},
	url = {https://aclanthology.org/D19-1077},
	doi = {10.18653/v1/D19-1077},
	abstract = {Pretrained contextual representation models (Peters et al., 2018; Devlin et al., 2018) have pushed forward the state-of-the-art on many NLP tasks. A new release of BERT (Devlin, 2018) includes a model simultaneously pretrained on 104 languages with impressive performance for zero-shot cross-lingual transfer on a natural language inference task. This paper explores the broader cross-lingual potential of mBERT (multilingual) as a zero shot language transfer model on 5 NLP tasks covering a total of 39 languages from various language families: NLI, document classification, NER, POS tagging, and dependency parsing. We compare mBERT with the best-published methods for zero-shot cross-lingual transfer and find mBERT competitive on each task. Additionally, we investigate the most effective strategy for utilizing mBERT in this manner, determine to what extent mBERT generalizes away from language specific features, and measure factors that influence cross-lingual transfer.},
	urldate = {2023-07-01},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Wu, Shijie and Dredze, Mark},
	month = nov,
	year = {2019},
	pages = {833--844},
	file = {Full Text PDF:/home/jirka/Zotero/storage/8HSSGUPL/Wu and Dredze - 2019 - Beto, Bentz, Becas The Surprising Cross-Lingual E.pdf:application/pdf},
}

@inproceedings{conneau_what_2018,
	address = {Melbourne, Australia},
	title = {What you can cram into a single \$\&!\#* vector: {Probing} sentence embeddings for linguistic properties},
	shorttitle = {What you can cram into a single \$\&!\#* vector},
	url = {https://aclanthology.org/P18-1198},
	doi = {10.18653/v1/P18-1198},
	abstract = {Although much effort has recently been devoted to training high-quality sentence embeddings, we still have a poor understanding of what they are capturing. “Downstream” tasks, often based on sentence classification, are commonly used to evaluate the quality of sentence representations. The complexity of the tasks makes it however difficult to infer what kind of information is present in the representations. We introduce here 10 probing tasks designed to capture simple linguistic features of sentences, and we use them to study embeddings generated by three different encoders trained in eight distinct ways, uncovering intriguing properties of both encoders and training methods.},
	urldate = {2023-07-03},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Conneau, Alexis and Kruszewski, German and Lample, Guillaume and Barrault, Loïc and Baroni, Marco},
	month = jul,
	year = {2018},
	pages = {2126--2136},
	file = {Full Text PDF:/home/jirka/Zotero/storage/HNEFSQLI/Conneau et al. - 2018 - What you can cram into a single \$&!# vector Prob.pdf:application/pdf},
}

@inproceedings{belinkov_interpretability_2020,
	address = {Online},
	title = {Interpretability and {Analysis} in {Neural} {NLP}},
	url = {https://aclanthology.org/2020.acl-tutorials.1},
	doi = {10.18653/v1/2020.acl-tutorials.1},
	abstract = {While deep learning has transformed the natural language processing (NLP) field and impacted the larger computational linguistics community, the rise of neural networks is stained by their opaque nature: It is challenging to interpret the inner workings of neural network models, and explicate their behavior. Therefore, in the last few years, an increasingly large body of work has been devoted to the analysis and interpretation of neural network models in NLP. This body of work is so far lacking a common framework and methodology. Moreover, approaching the analysis of modern neural networks can be difficult for newcomers to the field. This tutorial aims to fill this gap and introduce the nascent field of interpretability and analysis of neural networks in NLP. The tutorial will cover the main lines of analysis work, such as structural analyses using probing classifiers, behavioral studies and test suites, and interactive visualizations. We will highlight not only the most commonly applied analysis methods, but also the specific limitations and shortcomings of current approaches, in order to inform participants where to focus future efforts.},
	urldate = {2023-07-03},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {Tutorial} {Abstracts}},
	publisher = {Association for Computational Linguistics},
	author = {Belinkov, Yonatan and Gehrmann, Sebastian and Pavlick, Ellie},
	month = jul,
	year = {2020},
	pages = {1--5},
	file = {Full Text PDF:/home/jirka/Zotero/storage/3GECKG4R/Belinkov et al. - 2020 - Interpretability and Analysis in Neural NLP.pdf:application/pdf},
}

@inproceedings{blevins_analyzing_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Analyzing the {Mono}- and {Cross}-{Lingual} {Pretraining} {Dynamics} of {Multilingual} {Language} {Models}},
	url = {https://aclanthology.org/2022.emnlp-main.234},
	abstract = {The emergent cross-lingual transfer seen in multilingual pretrained models has sparked significant interest in studying their behavior. However, because these analyses have focused on fully trained multilingual models, little is known about the dynamics of the multilingual pretraining process. We investigate when these models acquire their in-language and cross-lingual abilities by probing checkpoints taken from throughout XLM-R pretraining, using a suite of linguistic tasks. Our analysis shows that the model achieves high in-language performance early on, with lower-level linguistic skills acquired before more complex ones. In contrast, the point in pretraining when the model learns to transfer cross-lingually differs across language pairs. Interestingly, we also observe that, across many languages and tasks, the final model layer exhibits significant performance degradation over time, while linguistic knowledge propagates to lower layers of the network. Taken together, these insights highlight the complexity of multilingual pretraining and the resulting varied behavior for different languages over time.},
	urldate = {2023-07-03},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Blevins, Terra and Gonen, Hila and Zettlemoyer, Luke},
	month = dec,
	year = {2022},
	pages = {3575--3590},
	file = {Full Text PDF:/home/jirka/Zotero/storage/B2XYT829/Blevins et al. - 2022 - Analyzing the Mono- and Cross-Lingual Pretraining .pdf:application/pdf},
}

@inproceedings{wolf_transformers_2020,
	address = {Online},
	title = {Transformers: {State}-of-the-{Art} {Natural} {Language} {Processing}},
	url = {https://www.aclweb.org/anthology/2020.emnlp-demos.6},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Rémi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and Platen, Patrick von and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Scao, Teven Le and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander M.},
	month = oct,
	year = {2020},
	pages = {38--45},
}

@inproceedings{pan_cross-lingual_2017,
	address = {Vancouver, Canada},
	title = {Cross-lingual {Name} {Tagging} and {Linking} for 282 {Languages}},
	url = {https://aclanthology.org/P17-1178},
	doi = {10.18653/v1/P17-1178},
	abstract = {The ambitious goal of this work is to develop a cross-lingual name tagging and linking framework for 282 languages that exist in Wikipedia. Given a document in any of these languages, our framework is able to identify name mentions, assign a coarse-grained or fine-grained type to each mention, and link it to an English Knowledge Base (KB) if it is linkable. We achieve this goal by performing a series of new KB mining methods: generating “silver-standard” annotations by transferring annotations from English to other languages through cross-lingual links and KB properties, refining annotations through self-training and topic selection, deriving language-specific morphology features from anchor links, and mining word translation pairs from cross-lingual links. Both name tagging and linking results for 282 languages are promising on Wikipedia data and on-Wikipedia data.},
	urldate = {2023-07-03},
	booktitle = {Proceedings of the 55th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Pan, Xiaoman and Zhang, Boliang and May, Jonathan and Nothman, Joel and Knight, Kevin and Ji, Heng},
	month = jul,
	year = {2017},
	pages = {1946--1958},
	file = {Full Text PDF:/home/jirka/Zotero/storage/TFR34WMJ/Pan et al. - 2017 - Cross-lingual Name Tagging and Linking for 282 Lan.pdf:application/pdf},
}

@article{artetxe_massively_2019,
	title = {Massively {Multilingual} {Sentence} {Embeddings} for {Zero}-{Shot} {Cross}-{Lingual} {Transfer} and {Beyond}},
	volume = {7},
	url = {https://aclanthology.org/Q19-1038},
	doi = {10.1162/tacl_a_00288},
	abstract = {We introduce an architecture to learn joint multilingual sentence representations for 93 languages, belonging to more than 30 different families and written in 28 different scripts. Our system uses a single BiLSTM encoder with a shared byte-pair encoding vocabulary for all languages, which is coupled with an auxiliary decoder and trained on publicly available parallel corpora. This enables us to learn a classifier on top of the resulting embeddings using English annotated data only, and transfer it to any of the 93 languages without any modification. Our experiments in cross-lingual natural language inference (XNLI data set), cross-lingual document classification (MLDoc data set), and parallel corpus mining (BUCC data set) show the effectiveness of our approach. We also introduce a new test set of aligned sentences in 112 languages, and show that our sentence embeddings obtain strong results in multilingual similarity search even for low- resource languages. Our implementation, the pre-trained encoder, and the multilingual test set are available at https://github.com/facebookresearch/LASER.},
	urldate = {2023-07-03},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Artetxe, Mikel and Schwenk, Holger},
	year = {2019},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	pages = {597--610},
	file = {Full Text PDF:/home/jirka/Zotero/storage/8YBPJ55Z/Artetxe and Schwenk - 2019 - Massively Multilingual Sentence Embeddings for Zer.pdf:application/pdf},
}

@inproceedings{rahimi_massively_2019,
	address = {Florence, Italy},
	title = {Massively {Multilingual} {Transfer} for {NER}},
	url = {https://aclanthology.org/P19-1015},
	doi = {10.18653/v1/P19-1015},
	abstract = {In cross-lingual transfer, NLP models over one or more source languages are applied to a low-resource target language. While most prior work has used a single source model or a few carefully selected models, here we consider a “massive” setting with many such models. This setting raises the problem of poor transfer, particularly from distant languages. We propose two techniques for modulating the transfer, suitable for zero-shot or few-shot learning, respectively. Evaluating on named entity recognition, we show that our techniques are much more effective than strong baselines, including standard ensembling, and our unsupervised method rivals oracle selection of the single best individual model.},
	urldate = {2023-07-03},
	booktitle = {Proceedings of the 57th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Rahimi, Afshin and Li, Yuan and Cohn, Trevor},
	month = jul,
	year = {2019},
	pages = {151--164},
	file = {Full Text PDF:/home/jirka/Zotero/storage/FNHQ7WYS/Rahimi et al. - 2019 - Massively Multilingual Transfer for NER.pdf:application/pdf},
}

@article{kuhn_hungarian_1955,
	title = {The {Hungarian} method for the assignment problem},
	volume = {2},
	number = {1-2},
	journal = {Naval research logistics quarterly},
	author = {Kuhn, Harold W},
	year = {1955},
	note = {Publisher: Wiley Online Library},
	pages = {83--97},
}

@article{gage_new_1994,
	title = {A new algorithm for data compression},
	volume = {12},
	issn = {0898-9788},
	number = {2},
	journal = {The C Users Journal},
	author = {Gage, Philip},
	month = feb,
	year = {1994},
	pages = {23--38},
}

@inproceedings{schuster_japanese_2012,
	title = {Japanese and {Korean} voice search},
	doi = {10.1109/ICASSP.2012.6289079},
	abstract = {This paper describes challenges and solutions for building a successful voice search system as applied to Japanese and Korean at Google. We describe the techniques used to deal with an infinite vocabulary, how modeling completely in the written domain for language model and dictionary can avoid some system complexity, and how we built dictionaries, language and acoustic models in this framework. We show how to deal with the difficulty of scoring results for multiple script languages because of ambiguities. The development of voice search for these languages led to a significant simplification of the original process to build a system for any new language which in in parts became our default process for internationalization of voice search.},
	booktitle = {2012 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Schuster, Mike and Nakajima, Kaisuke},
	month = mar,
	year = {2012},
	note = {ISSN: 2379-190X},
	keywords = {Decision support systems, Helium, Japanese, Korean, Speech recognition, voice search},
	pages = {5149--5152},
	file = {IEEE Xplore Abstract Record:/home/jirka/Zotero/storage/WZKNG5Q9/6289079.html:text/html;IEEE Xplore Full Text PDF:/home/jirka/Zotero/storage/QGFY38KH/Schuster and Nakajima - 2012 - Japanese and Korean voice search.pdf:application/pdf},
}

@misc{devlin_bertmultilingualmd_2019,
	title = {bert/multilingual.md at master · google-research/bert},
	url = {https://github.com/google-research/bert/blob/master/multilingual.md},
	abstract = {TensorFlow code and pre-trained models for BERT. Contribute to google-research/bert development by creating an account on GitHub.},
	language = {en},
	urldate = {2023-07-16},
	journal = {GitHub},
	author = {Devlin, Jacob},
	month = jul,
	year = {2019},
	file = {Snapshot:/home/jirka/Zotero/storage/9AICYPTE/multilingual.html:text/html},
}

@article{clark_canine_2022,
	title = {Canine: {Pre}-training an {Efficient} {Tokenization}-{Free} {Encoder} for {Language} {Representation}},
	volume = {10},
	issn = {2307-387X},
	shorttitle = {{\textless}span style="font-variant},
	url = {https://direct.mit.edu/tacl/article/doi/10.1162/tacl_a_00448/109284/Canine-Pre-training-an-Efficient-Tokenization-Free},
	doi = {10.1162/tacl_a_00448},
	abstract = {Pipelined NLP systems have largely been superseded by end-to-end neural modeling, yet nearly all commonly used models still require an explicit tokenization step. While recent tokenization approaches based on data-derived subword lexicons are less brittle than manually engineered tokenizers, these techniques are not equally suited to all languages, and the use of any fixed vocabulary may limit a model’s ability to adapt. In this paper, we present CANINE, a neural encoder that operates directly on character sequences—without explicit tokenization or vocabulary—and a pre-training strategy that operates either directly on characters or optionally uses subwords as a soft inductive bias. To use its finer-grained input effectively and efficiently, CANINE combines downsampling, which reduces the input sequence length, with a deep transformer stack, which encodes context. CANINE outperforms a comparable mBERT model by 5.7 F1 on TYDI QA, a challenging multilingual benchmark, despite having fewer model parameters.},
	language = {en},
	urldate = {2023-07-16},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Clark, Jonathan H. and Garrette, Dan and Turc, Iulia and Wieting, John},
	month = jan,
	year = {2022},
	pages = {73--91},
	file = {Clark et al. - 2022 - Caninespa.pdf:/home/jirka/Zotero/storage/5M7NZZNA/Clark et al. - 2022 - Caninespa.pdf:application/pdf},
}

@inproceedings{salesky_robust_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Robust {Open}-{Vocabulary} {Translation} from {Visual} {Text} {Representations}},
	url = {https://aclanthology.org/2021.emnlp-main.576},
	doi = {10.18653/v1/2021.emnlp-main.576},
	language = {en},
	urldate = {2023-07-16},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Salesky, Elizabeth and Etter, David and Post, Matt},
	year = {2021},
	pages = {7235--7252},
	file = {Salesky et al. - 2021 - Robust Open-Vocabulary Translation from Visual Tex.pdf:/home/jirka/Zotero/storage/6CSP7VKK/Salesky et al. - 2021 - Robust Open-Vocabulary Translation from Visual Tex.pdf:application/pdf},
}

@inproceedings{mansimov_towards_2020,
	address = {Online},
	title = {Towards {End}-to-{End} {In}-{Image} {Neural} {Machine} {Translation}},
	url = {https://www.aclweb.org/anthology/2020.nlpbt-1.8},
	doi = {10.18653/v1/2020.nlpbt-1.8},
	abstract = {In this paper, we offer a preliminary investigation into the task of in-image machine translation: transforming an image containing text in one language into an image containing the same text in another language. We propose an end-to-end neural model for this task inspired by recent approaches to neural machine translation, and demonstrate promising initial results based purely on pixel-level supervision. We then offer a quantitative and qualitative evaluation of our system outputs and discuss some common failure modes. Finally, we conclude with directions for future work.},
	language = {en},
	urldate = {2023-07-16},
	booktitle = {Proceedings of the {First} {International} {Workshop} on {Natural} {Language} {Processing} {Beyond} {Text}},
	publisher = {Association for Computational Linguistics},
	author = {Mansimov, Elman and Stern, Mitchell and Chen, Mia and Firat, Orhan and Uszkoreit, Jakob and Jain, Puneet},
	year = {2020},
	pages = {70--74},
	file = {Mansimov et al. - 2020 - Towards End-to-End In-Image Neural Machine Transla.pdf:/home/jirka/Zotero/storage/HPQEY6L4/Mansimov et al. - 2020 - Towards End-to-End In-Image Neural Machine Transla.pdf:application/pdf},
}

@inproceedings{zouhar_tokenization_2023,
	address = {Toronto, Canada},
	title = {Tokenization and the {Noiseless} {Channel}},
	url = {https://aclanthology.org/2023.acl-long.284},
	abstract = {Subword tokenization is a key part of most NLP pipelines.However, little is known about why some tokenizer and hyperparameter combinations lead to improved downstream model performance over others. We propose that good tokenizers lead to efficient channel usage, where the channel is the means by which some input is conveyed to the model and efficiency can be quantified in information-theoretic terms as the ratio of the Shannon entropy to the maximum entropy of the subword distribution.Nevertheless, an optimal encoding according to Shannon entropy assigns extremely long codes to low-frequency subwords and very short codes to high-frequency subwords.Defining efficiency in terms of Rényi entropy, on the other hand, penalizes distributions with either very high or very low-frequency subwords.We posit that (1) extremely high-frequency subwords are problematic because their meaning is not distinct and (2) that low-frequency subwords may not appear frequently enough for their meaning to be learned properly; encodings that induce unigram distributions with either can harm model performance.In machine translation, we find that across multiple tokenizers, the Rényi entropy has a very strong correlation with BLEU: 0.82 in comparison to just -0.30 for compressed length.},
	urldate = {2023-07-20},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Zouhar, Vilém and Meister, Clara and Gastaldi, Juan and Du, Li and Sachan, Mrinmaya and Cotterell, Ryan},
	month = jul,
	year = {2023},
	pages = {5184--5207},
	file = {Full Text PDF:/home/jirka/Zotero/storage/F7RDKKMQ/Zouhar et al. - 2023 - Tokenization and the Noiseless Channel.pdf:application/pdf},
}

@inproceedings{chung_improving_2020,
	address = {Online},
	title = {Improving {Multilingual} {Models} with {Language}-{Clustered} {Vocabularies}},
	url = {https://aclanthology.org/2020.emnlp-main.367},
	doi = {10.18653/v1/2020.emnlp-main.367},
	abstract = {State-of-the-art multilingual models depend on vocabularies that cover all of the languages the model will expect to see at inference time, but the standard methods for generating those vocabularies are not ideal for massively multilingual applications. In this work, we introduce a novel procedure for multilingual vocabulary generation that combines the separately trained vocabularies of several automatically derived language clusters, thus balancing the trade-off between cross-lingual subword sharing and language-specific vocabularies. Our experiments show improvements across languages on key multilingual benchmark tasks TyDi QA (+2.9 F1), XNLI (+2.1\%), and WikiAnn NER (+2.8 F1) and factor of 8 reduction in out-of-vocabulary rate, all without increasing the size of the model or data.},
	urldate = {2023-07-20},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Chung, Hyung Won and Garrette, Dan and Tan, Kiat Chuan and Riesa, Jason},
	month = nov,
	year = {2020},
	pages = {4536--4546},
	file = {Full Text PDF:/home/jirka/Zotero/storage/3K3BJXQW/Chung et al. - 2020 - Improving Multilingual Models with Language-Cluste.pdf:application/pdf},
}

@incollection{conneau_cross-lingual_2019,
	address = {Red Hook, NY, USA},
	title = {Cross-{Lingual} {Language} {Model} {Pretraining}},
	abstract = {Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsu-pervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9\% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT'16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT'16 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models are publicly available.},
	booktitle = {Proceedings of the 33rd {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Conneau, Alexis and Lample, Guillaume},
	year = {2019},
}

@inproceedings{lin_few-shot_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Few-shot {Learning} with {Multilingual} {Generative} {Language} {Models}},
	url = {https://aclanthology.org/2022.emnlp-main.616},
	abstract = {Large-scale generative language models such as GPT-3 are competitive few-shot learners. While these models are known to be able to jointly represent many different languages, their training data is dominated by English, potentially limiting their cross-lingual generalization. In this work, we train multilingual generative language models on a corpus covering a diverse set of languages, and study their few- and zero-shot learning capabilities in a wide range of tasks. Our largest model with 7.5 billion parameters sets new state of the art in few-shot learning in more than 20 representative languages, outperforming GPT-3 of comparable size in multilingual commonsense reasoning (with +7.4\% absolute accuracy improvement in 0-shot settings and +9.4\% in 4-shot settings) and natural language inference (+5.4\% in each of 0-shot and 4-shot settings). On the FLORES-101 machine translation benchmark, our model outperforms GPT-3 on 171 out of 182 directions with 32 training examples, while surpassing the official supervised baseline in 45 directions. We conduct an in-depth analysis of different multilingual prompting approaches, showing in particular that strong few-shot learning performance across languages can be achieved via cross-lingual transfer through both templates and demonstration examples.},
	urldate = {2023-07-20},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Lin, Xi Victoria and Mihaylov, Todor and Artetxe, Mikel and Wang, Tianlu and Chen, Shuohui and Simig, Daniel and Ott, Myle and Goyal, Naman and Bhosale, Shruti and Du, Jingfei and Pasunuru, Ramakanth and Shleifer, Sam and Koura, Punit Singh and Chaudhary, Vishrav and O'Horo, Brian and Wang, Jeff and Zettlemoyer, Luke and Kozareva, Zornitsa and Diab, Mona and Stoyanov, Veselin and Li, Xian},
	month = dec,
	year = {2022},
	pages = {9019--9052},
	file = {Full Text PDF:/home/jirka/Zotero/storage/FWPWQN48/Lin et al. - 2022 - Few-shot Learning with Multilingual Generative Lan.pdf:application/pdf},
}

@article{xue_byt5_2022,
	title = {{ByT5}: {Towards} a {Token}-{Free} {Future} with {Pre}-trained {Byte}-to-{Byte} {Models}},
	volume = {10},
	shorttitle = {{ByT5}},
	url = {https://aclanthology.org/2022.tacl-1.17},
	doi = {10.1162/tacl_a_00461},
	abstract = {Most widely used pre-trained language models operate on sequences of tokens corresponding to word or subword units. By comparison, token-free models that operate directly on raw text (bytes or characters) have many benefits: They can process text in any language out of the box, they are more robust to noise, and they minimize technical debt by removing complex and error-prone text preprocessing pipelines. Because byte or character sequences are longer than token sequences, past work on token-free models has often introduced new model architectures designed to amortize the cost of operating directly on raw text. In this paper, we show that a standard Transformer architecture can be used with minimal modifications to process byte sequences. We characterize the trade-offs in terms of parameter count, training FLOPs, and inference speed, and show that byte-level models are competitive with their token-level counterparts. We also demonstrate that byte-level models are significantly more robust to noise and perform better on tasks that are sensitive to spelling and pronunciation. As part of our contribution, we release a new set of pre-trained byte-level Transformer models based on the T5 architecture, as well as all code and data used in our experiments.1},
	urldate = {2023-07-20},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Xue, Linting and Barua, Aditya and Constant, Noah and Al-Rfou, Rami and Narang, Sharan and Kale, Mihir and Roberts, Adam and Raffel, Colin},
	year = {2022},
	note = {Place: Cambridge, MA
Publisher: MIT Press},
	pages = {291--306},
	file = {Full Text PDF:/home/jirka/Zotero/storage/EQ8DQXFP/Xue et al. - 2022 - ByT5 Towards a Token-Free Future with Pre-trained.pdf:application/pdf},
}

@inproceedings{rust_language_2023,
	title = {Language {Modelling} with {Pixels}},
	url = {https://openreview.net/forum?id=FkSp8VW8RjH},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Rust, Phillip and Lotz, Jonas F. and Bugliarello, Emanuele and Salesky, Elizabeth and Lhoneux, Miryam de and Elliott, Desmond},
	year = {2023},
}

@inproceedings{nivre_universal_2020,
	address = {Marseille, France},
	title = {Universal {Dependencies} v2: {An} {Evergrowing} {Multilingual} {Treebank} {Collection}},
	isbn = {979-10-95546-34-4},
	shorttitle = {Universal {Dependencies} v2},
	url = {https://aclanthology.org/2020.lrec-1.497},
	abstract = {Universal Dependencies is an open community effort to create cross-linguistically consistent treebank annotation for many languages within a dependency-based lexicalist framework. The annotation consists in a linguistically motivated word segmentation; a morphological layer comprising lemmas, universal part-of-speech tags, and standardized morphological features; and a syntactic layer focusing on syntactic relations between predicates, arguments and modifiers. In this paper, we describe version 2 of the universal guidelines (UD v2), discuss the major changes from UD v1 to UD v2, and give an overview of the currently available treebanks for 90 languages.},
	language = {English},
	urldate = {2023-07-20},
	booktitle = {Proceedings of the {Twelfth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Nivre, Joakim and de Marneffe, Marie-Catherine and Ginter, Filip and Hajič, Jan and Manning, Christopher D. and Pyysalo, Sampo and Schuster, Sebastian and Tyers, Francis and Zeman, Daniel},
	month = may,
	year = {2020},
	pages = {4034--4043},
	file = {Full Text PDF:/home/jirka/Zotero/storage/LZQBW2UJ/Nivre et al. - 2020 - Universal Dependencies v2 An Evergrowing Multilin.pdf:application/pdf},
}

@inproceedings{tay_charformer_2022,
	title = {Charformer: {Fast} {Character} {Transformers} via {Gradient}-based {Subword} {Tokenization}},
	url = {https://openreview.net/forum?id=JtBRnrlOEFN},
	booktitle = {International {Conference} on {Learning} {Representations}},
	author = {Tay, Yi and Tran, Vinh Q. and Ruder, Sebastian and Gupta, Jai and Chung, Hyung Won and Bahri, Dara and Qin, Zhen and Baumgartner, Simon and Yu, Cong and Metzler, Donald},
	year = {2022},
}

@inproceedings{limisiewicz_tokenization_2023,
	address = {Toronto, Canada},
	title = {Tokenization {Impacts} {Multilingual} {Language} {Modeling}: {Assessing} {Vocabulary} {Allocation} and {Overlap} {Across} {Languages}},
	shorttitle = {Tokenization {Impacts} {Multilingual} {Language} {Modeling}},
	url = {https://aclanthology.org/2023.findings-acl.350},
	abstract = {Multilingual language models have recently gained attention as a promising solution for representing multiple languages in a single model. In this paper, we propose new criteria to evaluate the quality of lexical representation and vocabulary overlap observed in sub-word tokenizers.Our findings show that the overlap of vocabulary across languages can be actually detrimental to certain downstream tasks (POS, dependency tree labeling). In contrast, NER and sentence-level tasks (cross-lingual retrieval, NLI) benefit from sharing vocabulary. We also observe that the coverage of the language-specific tokens in the multilingual vocabulary significantly impacts the word-level tasks. Our study offers a deeper understanding of the role of tokenizers in multilingual language models and guidelines for future model developers to choose the most suitable tokenizer for their specific application before undertaking costly model pre-training.},
	urldate = {2023-07-20},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {ACL} 2023},
	publisher = {Association for Computational Linguistics},
	author = {Limisiewicz, Tomasz and Balhar, Jiří and Mareček, David},
	month = jul,
	year = {2023},
	pages = {5661--5681},
	file = {Full Text PDF:/home/jirka/Zotero/storage/3YSCUXHD/Limisiewicz et al. - 2023 - Tokenization Impacts Multilingual Language Modelin.pdf:application/pdf},
}

@inproceedings{vaswani_attention_2017,
	title = {Attention is {All} you {Need}},
	volume = {30},
	url = {https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, Łukasz and Polosukhin, Illia},
	editor = {Guyon, I. and Luxburg, U. Von and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
	year = {2017},
}

@article{radford_improving_2018-1,
	title = {Improving language understanding by generative pre-training},
	author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and {others}},
	year = {2018},
	note = {Publisher: OpenAI},
}

@inproceedings{hu_xtreme_2020,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {{XTREME}: {A} {Massively} {Multilingual} {Multi}-task {Benchmark} for {Evaluating} {Cross}-lingual {Generalisation}},
	volume = {119},
	url = {https://proceedings.mlr.press/v119/hu20b.html},
	abstract = {Much recent progress in applications of machine learning models to NLP has been driven by benchmarks that evaluate models across a wide variety of tasks. However, these broad-coverage benchmarks have been mostly limited to English, and despite an increasing interest in multilingual models, a benchmark that enables the comprehensive evaluation of such methods on a diverse range of languages and tasks is still missing. To this end, we introduce the Cross-lingual TRansfer Evaluation of Multilingual Encoders (XTREME) benchmark, a multi-task benchmark for evaluating the cross-lingual generalization capabilities of multilingual representations across 40 languages and 9 tasks. We demonstrate that while models tested on English reach human performance on many tasks, there is still a sizable gap in the performance of cross-lingually transferred models, particularly on syntactic and sentence retrieval tasks. There is also a wide spread of results across languages. We will release the benchmark to encourage research on cross-lingual learning methods that transfer linguistic knowledge across a diverse and representative set of languages and tasks.},
	booktitle = {Proceedings of the 37th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Hu, Junjie and Ruder, Sebastian and Siddhant, Aditya and Neubig, Graham and Firat, Orhan and Johnson, Melvin},
	editor = {III, Hal Daumé and Singh, Aarti},
	month = jul,
	year = {2020},
	pages = {4411--4421},
}

@inproceedings{wenzek_ccnet_2020,
	address = {Marseille, France},
	title = {{CCNet}: {Extracting} {High} {Quality} {Monolingual} {Datasets} from {Web} {Crawl} {Data}},
	isbn = {979-10-95546-34-4},
	shorttitle = {{CCNet}},
	url = {https://aclanthology.org/2020.lrec-1.494},
	abstract = {Pre-training text representations have led to significant improvements in many areas of natural language processing. The quality of these models benefits greatly from the size of the pretraining corpora as long as its quality is preserved. In this paper, we describe an automatic pipeline to extract massive high-quality monolingual datasets from Common Crawl for a variety of languages. Our pipeline follows the data processing introduced in fastText (Mikolov et al., 2017; Grave et al., 2018), that deduplicates documents and identifies their language. We augment this pipeline with a filtering step to select documents that are close to high quality corpora like Wikipedia.},
	language = {English},
	urldate = {2023-07-20},
	booktitle = {Proceedings of the {Twelfth} {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Wenzek, Guillaume and Lachaux, Marie-Anne and Conneau, Alexis and Chaudhary, Vishrav and Guzmán, Francisco and Joulin, Armand and Grave, Edouard},
	month = may,
	year = {2020},
	pages = {4003--4012},
	file = {Full Text PDF:/home/jirka/Zotero/storage/H4LZFQE2/Wenzek et al. - 2020 - CCNet Extracting High Quality Monolingual Dataset.pdf:application/pdf},
}
