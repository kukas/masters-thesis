\begin{table}
\caption{In this summary table, we present all tokenizers used in this chapter. In the table, we include the tokenizers obtained by replicating the papers \citet{chung_improving_2020,zheng_allocating_2021,liang_xlm-v_2023} in our setting. We also include the Huggingface tokenizers from \autoref{tab:20l_metrics} and Sentencepiece Unigram tokenizers from \autoref{tab:data_balance_metrics}. As we can see, the Huggingface Unigram tokenizer is a clear outlier in terms of all metrics even after taking in account the higher alphabet size as explored in \autoref{tab:coverage_influence}. Further, we can see that the clustering methods with a higher number of clusters are improving over the baselines the authors used (\textit{Unigram, $\alpha$=0.5} and \textit{Unigram, $\alpha$=0.7}). On the other hand, we see that using more balanced data for training the Sentencepiece Unigram (\textit{Unigram, $\alpha$=0.0}) leads to better overall performance compared to the replicated methods. We note that the alphabet sizes for all relevant tokenizers stay in the stable range of 1000-5000 so we do not expect this variable to influence the tokenizer metrics. The rows are sorted by the CPT score.}
\label{tab:all_tokenizers_metrics}
\begin{tabular}{lrrrrr}
\toprule
Tokenizer & Alphabet & \# UNKs & CPT & AR & JSD \\
\midrule
Hugg. BPE, $\alpha$=0.25 & 1000 & 14040.1 & 3.713 & 1253.7 & 0.783 \\
Unigram, $\alpha$=0.0 & 2975 & 617.1 & 3.712 & 1212.9 & 0.767 \\
Chung 20 clusters & 4123 & 270.3 & 3.702 & 1098.7 & 0.766 \\
Unigram, $\alpha$=0.3 & 2666 & 923.5 & 3.702 & 1190.7 & 0.768 \\
TokMix, $\alpha$=0.25 & 2497 & 1203.2 & 3.691 & 1163.4 & 0.773 \\
Chung 16 clusters & 3933 & 387.1 & 3.677 & 1102.2 & 0.767 \\
Liang 20 clusters & 3709 & 341.4 & 3.676 & 1103.2 & 0.765 \\
Zheng 20langs & 4854 & 245.7 & 3.673 & 1094.5 & 0.765 \\
Liang 16 clusters & 3655 & 416.8 & 3.669 & 1106.2 & 0.767 \\
Sentpiece. BPE, $\alpha$=0.25 & 1215 & 7235.6 & 3.666 & 1212.9 & 0.774 \\
Unigram, $\alpha$=0.5 & 2859 & 729.0 & 3.618 & 1143.8 & 0.769 \\
Chung 8 clusters & 4870 & 684.4 & 3.575 & 1061.1 & 0.770 \\
Unigram, $\alpha$=0.7 & 2733 & 883.2 & 3.556 & 1107.1 & 0.770 \\
Chung 4 clusters & 3253 & 648.6 & 3.546 & 1071.9 & 0.768 \\
Liang 8 clusters & 4283 & 568.2 & 3.544 & 1081.6 & 0.767 \\
Liang 4 clusters & 3698 & 419.2 & 3.512 & 1082.5 & 0.769 \\
Unigram, $\alpha$=1.0 & 2476 & 1286.3 & 3.442 & 1041.8 & 0.772 \\
Hugg. unigram, $\alpha$=0.25 & 12616 & 4.5 & 3.204 & 1010.5 & 0.745 \\
\bottomrule
\end{tabular}
\end{table}
