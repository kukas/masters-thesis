\begin{table}
\caption{We train five Sentencepiece Unigram tokenizers on increasingly imbalanced multilingual dataset. We see that the macro averaged metrics decrease with the increasing imbalance, suggesting that on average, the tokenizer represents the languages less well.}
\label{tab:data_balance_metrics}
\begin{tabular}{lrrrrr}
\toprule
Tokenizer & Alphabet & \# UNKs & CPT & AR & JSD \\
\midrule
Unigram $\alpha$=0.0 & 2975 & 617.1 & 3.712 & 1212.9 & 0.767 \\
Unigram $\alpha$=0.3 & 2666 & 923.5 & 3.702 & 1190.7 & 0.768 \\
Unigram $\alpha$=0.5 & 2859 & 729.0 & 3.618 & 1143.8 & 0.769 \\
Unigram $\alpha$=0.7 & 2733 & 883.2 & 3.556 & 1107.1 & 0.770 \\
Unigram $\alpha$=1.0 & 2476 & 1286.3 & 3.442 & 1041.8 & 0.772 \\
\bottomrule
\end{tabular}
\end{table}
