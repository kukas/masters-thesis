\begin{table}
\caption{We measure how much data is generally needed for the tokenizer training. We train handful of Sentencepiece Unigram tokenizers on different amounts of balanced multilingual data. We observe that after 100k-1M lines per language, the tokenizers converge to similar vocabulary allocation and overlap scores.}
\label{tab:data_size_influence}
\begin{tabular}{rrrrrr}
\toprule
Lines per lang. & Alphabet & \# UNKs & CPT & AR & JSD \\
\midrule
1\,000 & 3598 & 520.4 & 3.302 & 958.4 & 0.766 \\
10\,000 & 4725 & 117.8 & 3.598 & 1089.1 & 0.765 \\
100\,000 & 5041 & 65.5 & 3.696 & 1192.2 & 0.767 \\
1\,000\,000 & 5079 & 62.6 & 3.702 & 1204.7 & 0.767 \\
1\,500\,000 & 5176 & 55.9 & 3.705 & 1210.7 & 0.767 \\
2\,000\,000 & 5180 & 56.4 & 3.705 & 1212.5 & 0.767 \\
\bottomrule
\end{tabular}
\end{table}
